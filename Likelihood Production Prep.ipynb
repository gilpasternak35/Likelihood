{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for project purposes\n",
    "# Full Project imports\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import dateutil\n",
    "from datetime import datetime, timedelta\n",
    "import requests as rd\n",
    "import numpy as np\n",
    "from sklearn import neighbors, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import smtplib\n",
    "import scipy.stats as st\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anomaly:\n",
    "    '''Base Class for an anomaly detection method'''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    \n",
    "    # Constructor to set values for data\n",
    "    def __init__(self, input_data = None):\n",
    "        '''\n",
    "        Constructor for setting dataset reference to a specific dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data: Pandas DataFrame reference - Your dataset in the form of a Pandas DataFrame\n",
    "        '''\n",
    "        # Ensuring data is properly formatted\n",
    "        assert input_data is None or type(input_data) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.data  = input_data\n",
    "        \n",
    "        \n",
    "    # Loading data into project\n",
    "    def load_html(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an HTML table and sets it as the dataset for the model.\n",
    "        \n",
    "        Common issues: inputting an invalid file path (your file will not be read if this is the case),\n",
    "        linking another file format (ensure that your link is indeed a link to a website with tables), or giving a \n",
    "        link to a website which does not allow scraping of its information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_html(link)\n",
    "        return self.data    \n",
    "    \n",
    "    \n",
    "    # Loading data into project\n",
    "    def load_csv(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an CSV table and sets it as the dataset for the model. \n",
    "        \n",
    "        Common issues: Incorrect file path (ensure your file path is valid), a failure to enter a valid CSV\n",
    "        (ensure your file is in CSV format)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_csv(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_excel(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an Exel table and sets it as the dataset for the model.\n",
    "        \n",
    "        Common issues: Incorrect file path (ensure your file path is valid), a failure to enter a valid Excel\n",
    "        (ensure your file is in Excel format), Random spaces within your data (A random space within an Excel file might be read as an NaN value)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_excel(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_sql_table(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads a SQL table and sets it as the dataset for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_sql_table(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Setter for the training set\n",
    "    def set_train(self, trainingSet: pd.DataFrame):\n",
    "        '''\n",
    "        A setter for the training set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainingSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        '''\n",
    "        self.trainDf = trainingSet\n",
    "    \n",
    "    \n",
    "    # Setter for the test set\n",
    "    def set_test(self, testSet: pd.DataFrame):\n",
    "        '''\n",
    "        A setter for the test set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        testSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        '''\n",
    "        self.testDf = testSet\n",
    "        \n",
    "    \n",
    "    # Randomly split train and test set\n",
    "    def assign_train_test(self,random_state = 42, training_set_ratio = 0.8, shuffling = True):\n",
    "        '''\n",
    "        A default random splitter into train and test set\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int - determines random state fed to model for reproducability of random results, default is 42\n",
    "        training_set_ratio: float between 0.0 and 1.0 - what % of your data you would like to encompass the training set (test set will be made in complimentary way) default is 0.8\n",
    "        shuffling: boolean - whether or not you would like your data randomly shuffled out of chronology prior to split (True/False). Default is True.\n",
    "        '''\n",
    "        # Ensuring that data actually exists before splitting\n",
    "        assert not(self.data is None), \"You cannot assign a train and test set out of a dataset that has not been initialized\"\n",
    "        \n",
    "        # Splitting into train and test\n",
    "        self.trainDf, self.testDf = train_test_split(self.data, train_size = training_set_ratio, shuffle = shuffling)\n",
    "        return self.trainDf, self.testDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrap(Anomaly):\n",
    "    '''A class for returning anomaly of categorical column counts, utilizing the metric of surprise (entropy)'''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    timestamp = None\n",
    "    params = None\n",
    "    \n",
    "    # Overloaded constructor in case user doesn't want to fit data right away\n",
    "    def __init__(self, timeCol = 'date_time', resamples = 1000, maxTrainingSizeMult = 10, maxCategory = 100, minCategories = 10):\n",
    "        '''\n",
    "        Constructor which does not require immediate fit to model, merely initializes timestamp if given\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timeCol: String - The name of the primary TimeStamp column\n",
    "        resamples: int - the number of times the bootstrap resamples. Making this very large will improve accuracy but significantly lower speed. Default = 1000\n",
    "        maxTrainingSizeMult: int - If there is more than x  = maxTrainingSizeMult ratio of training to test data, trim training data to most recent. Default = 10\n",
    "        maxCategory: int - Maximum number of categories in a column (to ensure that counts are not tiny and are meaninful), column skipped if value count higher than this. Default = 100\n",
    "        minCategory: int - if column has a category count that is lower than this value, don't report it in bootstrap surprise. Default = 10.\n",
    "        '''\n",
    "        # Initializing time\n",
    "        timestamp = timeCol\n",
    "        \n",
    "        # Meta-parameter initialization\n",
    "        params = {\n",
    "          \"bootstrapResamples\": resamples,\n",
    "          \"maxTrainingSizeMultiple\":maxTrainingSizeMult, # if there is more than X times more training data, trim to most recent\n",
    "          \"maxCategories\":maxCategory,\n",
    "          \"minCategoryCount\": minCategories,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # Fot fitting data right away\n",
    "    def __init__(self, dataset, timeCol = \"date_time\",  resamples = 1000, maxTrainingSizeMult = 10, maxCategory = 100, minCategories = 10):\n",
    "        '''\n",
    "        Overloaded constructor for attaching dataset immediately, can be done independently within any of the load functions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: String - A pandas data frame reference\n",
    "        timeCol: String - The name of the primary TimeStamp column. Default = \"date_time\".\n",
    "        resamples: int - the number of times the bootstrap resamples. Making this very large will improve accuracy but significantly lower speed. Default = 1000\n",
    "        maxTrainingSizeMult: int - If there is more than x  = maxTrainingSizeMult ratio of training to test data, trim training data to most recent. Default = 10\n",
    "        maxCategory: int - Maximum number of categories in a column (to ensure that counts are not tiny and are meaninful), column skipped if value count higher than this. Default = 100\n",
    "        minCategory: int - if column has a category count that is lower than this value, don't report it in bootstrap surprise. Default = 10.\n",
    "        '''\n",
    "        timestamp = timeCol\n",
    "        data = dataset\n",
    "        \n",
    "        # Meta-parameter initialization\n",
    "        params = {\n",
    "          \"bootstrapResamples\": resamples,\n",
    "          \"maxTrainingSizeMultiple\":maxTrainingSizeMult, # if there is more than X times more training data, trim to most recent\n",
    "          \"maxCategories\":maxCategory,\n",
    "          \"minCategoryCount\": minCategories,\n",
    "        }\n",
    "        \n",
    "    \n",
    "    # Converts Timetamp column of DataFrame to a legitimate timestamp\n",
    "    def convert_time_stamp_to_datetime(self: str, formatting = '%Y%m%d %H:%M:%S') -> pd.DataFrame:\n",
    "        '''\n",
    "        Converts a chosen timestamp column from string to date/time, making the modifications both to the fitted\n",
    "        Data Frame and returning the new Data Frame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp: String - The name of the Timestamp column that needs conversion\n",
    "        formatting: String - If formatting different from default = %Y%m%d %H:%M:%S, enter the format of your TimeSeries column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the locally the entire DataFrame with the modified Timestamp column\n",
    "        '''\n",
    "        self.data[timestamp] =  pd.to_datetime(self.data[timestamp], format = formatting)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(batchHours = 24*7):\n",
    "        '''\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        '''\n",
    "        maxTs = max(self.data[timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[timestamp] < batchTs]\n",
    "        \n",
    " \n",
    "    # Helpers and Math\n",
    "    def pValue(self,data, threshold: np.number, result: pd.DataFrame) -> np.array:\n",
    "        '''\n",
    "        Returns the p-value of a computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Pandas DataFrame - The Data we are computing the P-value on\n",
    "        threshold: np.number - The threshold to check if data is anomalous\n",
    "        result: pd.DataFrame - A DataFrame containing the column \"Bootstrap counts\" to be normalized and tested for anomaly\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pGauss : np.array\n",
    "            Returns the array of normalized p-values for each bootstrap count\n",
    "        '''\n",
    "        # Taking the smaller of the 2 p-values(either could present large anomaly)\n",
    "        pLarger = sum(np.array(data) >= threshold) / len(data)\n",
    "        pSmaller = sum(np.array(data) <= threshold) / len(data)\n",
    "        p = min(pLarger, pSmaller)\n",
    "\n",
    "        # only use gaussian p-value when there is variation, but bootsrap p = 0\n",
    "        stdev = np.std(data)\n",
    "        if stdev == 0 or p != 0:\n",
    "            pGauss = p\n",
    "        else:\n",
    "            # Normalizing\n",
    "            pGauss = st.norm(np.mean(result['bootstrap_counts']), stdev).cdf(result['count'])\n",
    "            pGauss = min(pGauss,1-pGauss)\n",
    "        return pGauss\n",
    "\n",
    "    \n",
    "    def trimFrame(self,df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Trims a DataFrame, ensuring that it does not exceed the training set max size hyper parameter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame that is being trimmed to fit to the training set hyperparameter\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dfTrimmed : pandas DataFrame\n",
    "            Returns a DataFrame fit to the training set specifications\n",
    "        '''\n",
    "        # trim to most recent\n",
    "        df = df.sort_values(self.timestamp, ascending =False)\n",
    "        dfTrimmed = df[:self.params['maxTrainingSizeMultiple']*len(testDf)]\n",
    "\n",
    "        return dfTrimmed\n",
    "    \n",
    "    \n",
    "    # Returns names of categorical columns\n",
    "    def getCategoricalColumnNames(df: pd.DataFrame) -> []:\n",
    "        '''\n",
    "        Returns the names of categorical columns in a Pandas DataFrame (if the type is a string)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame whose columns are checked for being categorical data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        columnNames : list\n",
    "            The list of all categorical column names \n",
    "        '''\n",
    "        columnNames = []\n",
    "        for columnName in df.keys():\n",
    "            if (type (df[columnName].iloc[0])) == str:\n",
    "                columnNames.append(columnName)\n",
    "        return columnNames\n",
    "    \n",
    "    \n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        Tests for difference between training and test set counts, returning a report that quantifies difference between\n",
    "        training and test set as surprise.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        resultsDf : pandas DataFrame\n",
    "            A DataFrame containing a report for the difference between expected and detected counts within the test set \n",
    "            With the inclusion of a column quantifying irregularity as surprise (entropy)\n",
    "        \n",
    "        '''\n",
    "        # Preventative measures\n",
    "        assert self.trainDf != None and self.testDf != None, \"Please set up your train and test sets prior to attempting this step\"\n",
    "        \n",
    "        # get all of the string columns\n",
    "        columnNames = self.getCategoricalColumnNames(self.testDf)\n",
    "\n",
    "        bootstrapDf = self.trimFrame(self.trainDf)\n",
    "\n",
    "        # set up dict, add counts\n",
    "        results = {}\n",
    "\n",
    "\n",
    "        for columnName in columnNames:\n",
    "\n",
    "            # if it isn't a string column, reject it\n",
    "            if type(testDf[columnName].iloc[0]) != str:\n",
    "                continue\n",
    "            categories = (bootstrapDf[columnName].append(self.testDf[columnName])).unique()\n",
    "            if len(categories) > self.params['maxCategories']:\n",
    "                continue\n",
    "\n",
    "            results[columnName] = {}\n",
    "            testCounts = self.testDf[columnName].value_counts(dropna = False)\n",
    "            \n",
    "            \n",
    "            for i in np.arange(1,len(categories) -1):\n",
    "                if(pd.isna(categories[i])):\n",
    "                    categories = np.delete(categories, i)  \n",
    "            for category in categories:\n",
    "                results[columnName][category] = {'bootstrap_counts':[],\n",
    "\n",
    "                                                 'count':testCounts.get(category,0)}\n",
    "        # resample, add boostrap counts\n",
    "        for ii in range(params['bootstrapResamples']):\n",
    "\n",
    "            # Draw random sample from training\n",
    "            sampleDf = bootstrapDf.sample(len(testDf), replace=True)\n",
    "            for columnName in results.keys():\n",
    "\n",
    "                # count by category\n",
    "                trainCounts = sampleDf[columnName].value_counts(dropna = False)\n",
    "\n",
    "                # put results in dict\n",
    "                for category in results[columnName].keys():\n",
    "                    boostrapCount = trainCounts.get(category,0)\n",
    "                    results[columnName][category]['bootstrap_counts'].append(boostrapCount)\n",
    "\n",
    "        # convert to records, add p-values\n",
    "        bootstrap_results = []\n",
    "        for columnName in results.keys():\n",
    "            for category in results[columnName].keys():\n",
    "                result = results[columnName][category]\n",
    "\n",
    "                estimatedCount = int(np.round(np.mean(result['bootstrap_counts'])))\n",
    "\n",
    "                # don't report entries with very low predicted and actual counts\n",
    "                if estimatedCount < params['minCategoryCount'] and result['count'] < params['minCategoryCount']:\n",
    "                    continue\n",
    "\n",
    "                p = pValue(result['bootstrap_counts'],result['count'], result)\n",
    "                categoryName = category\n",
    "\n",
    "                # Backup\n",
    "                if not category:\n",
    "                    categoryName = \"NULL\"\n",
    "\n",
    "                bootstrap_results.append({\"column\":columnName,\n",
    "                                   \"category\":categoryName,\n",
    "                                   \"count\":result['count'],\n",
    "                                   \"p\": p,\n",
    "                                   \"estimated_count\":estimatedCount,\n",
    "                                   })\n",
    "\n",
    "        # Sorting by P-values and obtaining Surprise of each\n",
    "        if(np.count_nonzero(p)>0):\n",
    "            resultsDf = pd.DataFrame.from_records(bootstrap_results).sort_values('p')\n",
    "            resultsDf['surprise'] = -np.log2(resultsDf['p'])\n",
    "\n",
    "            return resultsDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fbprophet:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "class TimeSeries(Anomaly):\n",
    "    '''\n",
    "    Utilizes facebook prophet and its ability to predict the future based off specific time context (day, hour, holiday)\n",
    "    to make predictions and test those against the dataset, thus finding anomaly with the context of time. Please ensure \n",
    "    you set your train and test sets prior to computation.\n",
    "    '''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    prophetDf = None\n",
    "    countryModel = None\n",
    "    forecast = None\n",
    "    timestamp = None\n",
    "    \n",
    "    def __init__(self,timeStampInput = 'date_time', inp_data = None, train = None,test = None):\n",
    "        '''\n",
    "        Does not require any inputs, but gives user option to initialize input data/train/test right from the get-go\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_data: pandas DataFrame - A given data set\n",
    "        train: pandas DataFrame - A given training set\n",
    "        test: pandas DataFrame - A given test set\n",
    "        '''\n",
    "        \n",
    "        # Ensuring that if user has given us input, it is of the correct form\n",
    "        assert input_data is None or type(input_data) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        data  = inp_data\n",
    "        \n",
    "        # Ensuring training set is of the correct form\n",
    "        assert train is None or type(train) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        trainDf  = train\n",
    "        \n",
    "        # Ensuring test set is of the correct form\n",
    "        assert test is None or type(test) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        testDf = test\n",
    "        \n",
    "        \n",
    "        timestamp = timeStampInput\n",
    "        \n",
    "        \n",
    "            \n",
    "    def truncateTs(ts: pd.Series) -> pd.Series:\n",
    "        '''\n",
    "        Truncates a timestamp column to the hour percision (minute, second, and microsecond all set to 0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ts: pandas Series - A given Timestamp column to be truncated\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ts : pandas Series\n",
    "            The Timestamp column truncated to the hour percision\n",
    "        '''\n",
    "        return ts.replace(minute=0, second=0,  microsecond=0)\n",
    "    \n",
    "    \n",
    "    def group_and_build_time_table(self,truncated = False) -> pd.DataFrame:\n",
    "        '''\n",
    "        Builds a table on the basis of the value counts (or rather the log 10 of the value counts)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        truncated: boolean - A value representing whether or not the training set has already been truncated to the hour. Default is False. If this is the case, the data will be autotruncated. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prophetDf : pandas Data Frame\n",
    "                   A table with value grouping by value counts\n",
    "        \n",
    "        '''\n",
    "        # Making a copy as to not mess up reference\n",
    "        truncatedData  = self.trainDf.copy()\n",
    "        \n",
    "        # Truncating timestamp if needed\n",
    "        if truncated == False:\n",
    "            truncatedData[timestamp] = truncateTs(truncatedData[timestamp])\n",
    "        groupedCounts = truncatedData.value_counts()\n",
    "        \n",
    "        # Grouping counts in a single DataFrame\n",
    "        self.prophetDf = pd.DataFrame({'ds':groupedCounts.index,'y':np.log10(groupedCounts.values)})\n",
    "        return self.prophetDf\n",
    "    \n",
    "    \n",
    "    # Takes in the the dataset and the prophet dataset returned by the ast option\n",
    "    def train_model_on_country(self, country = \"US\"):\n",
    "        '''\n",
    "        Trains a Facebook Prophet model on a specified country with a linear growth algorithm and an interval\n",
    "        width of one sigma. Default country is the United States. Will fit this country model onto TimeSeries table.\n",
    "        \n",
    "        Common issues: Downloading Prophet can be very messy and certain modifications might need to be made to \n",
    "        enable holidays such as Easter. You can read more about this issue here: \n",
    "        \n",
    "        https://github.com/facebook/prophet/issues/1293\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Country: String - The name of a valid country included in the Prophet seasonality package\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.countryModel: Table\n",
    "                          Facebook Prophet model fitted onto the country of your choice (is also now contained as\n",
    "                          an instance variable)\n",
    "        \n",
    "        '''\n",
    "        # Ensuring inputted country is a string\n",
    "        assert type(country) == str, \"Given country should be formatted as a string\"\n",
    "        # Train model\n",
    "        self.countryModel = Prophet(#daily_seasonality = True, \n",
    "                    #yearly_seasonality = False, \n",
    "                    #weekly_seasonality = True, \n",
    "                    #growth='linear',\n",
    "                    interval_width=0.68 # one sigma\n",
    "                   )\n",
    "        self.countryModel.add_country_holidays(country_name=country)\n",
    "\n",
    "        self.countryModel.fit(prophetDf)\n",
    "        return self.countryModel\n",
    "    \n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(batchHours = 24*7):\n",
    "        '''\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        '''\n",
    "        maxTs = max(self.data[timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[timestamp] < batchTs]\n",
    "        \n",
    "\n",
    "    # Applies Prophet analytics to create a forecast based on hours\n",
    "    def predict_future(self, timestamp = \"date_time\"):\n",
    "        '''\n",
    "        Builds (and returns) a future forecast for comparison to test set (which should be further ahead in time relative \n",
    "        to the trainig set). Made on the basis of the number of hours which encompass the test set.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.forecast: Table\n",
    "                      A forecast representiative of the predictions the Prophet Model would assume the test set to be,\n",
    "                      based on the training set.\n",
    "        '''\n",
    "\n",
    "        # Takes in trained model and predicts the future\n",
    "        # find number of hours to preduct: ceil of hours in testDf\n",
    "        \n",
    "        # Obtaining interval contained by test set for computation purposes.\n",
    "        timeDelta = max(self.testDf[timestamp]) - min(self.testDf[timestamp])\n",
    "\n",
    "        #If a column is string, convert to date/time\n",
    "        if(testDf.applymap(type).eq(str).any()[timestamp]):\n",
    "            testDf['ts'] = pd.to_datetime(testDf[timestamp])\n",
    "\n",
    "        timeDelta = max(testDf[timestamp]) -min(testDf[timestamp])\n",
    "        hours = int(timeDelta.days*24 + timeDelta.seconds/(60*60))+1\n",
    "        future = self.countryModel.make_future_dataframe(periods = hours, freq = 'H')\n",
    "        self.forecast = self.countryModel.predict(future)\n",
    "        return self.forecast\n",
    "\n",
    "    \n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        Based on the training-set-reliant prediction of the future, calculates the anomaly between the training and the\n",
    "        test set utilizing the metric of surprise.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prophetResultsDf: Pandas DataFrame\n",
    "                      A table containing difference between observed and expected counts, sorted in order of the metric of surprise\n",
    "                      (How anomalous/chaotic/entropic the data is)\n",
    "        '''\n",
    "        groupedCounts = self.trainDf.value_counts()\n",
    "\n",
    "        prophetTestDf = pd.DataFrame({'ds':groupedCounts.index,\n",
    "                                      'y':np.log10(groupedCounts.values),\n",
    "                                      'y_linear':groupedCounts.values})\n",
    "\n",
    "        # find p-value\n",
    "        prophet_results = []\n",
    "\n",
    "        # Comparing test and training set data for identical intervals\n",
    "        for ii in range(len(prophetTestDf)):\n",
    "            ts = prophetTestDf['ds'][ii]\n",
    "            fcstExample = forecast[forecast['ds'] == ts]\n",
    "            mean = fcstExample['yhat'].iloc[0]\n",
    "            stdev = (fcstExample['yhat_upper'].iloc[0] - fcstExample['yhat_lower'].iloc[0])/2\n",
    "\n",
    "            # Calculating the P-value\n",
    "            p = st.norm(mean, stdev).cdf(prophetTestDf['y'][ii])\n",
    "            p = min(p,1-p)\n",
    "\n",
    "            prophet_results.append({\"column\":\"Forecast\",\n",
    "                               \"category\":str(ts),\n",
    "                               \"count\":prophetTestDf['y_linear'][ii],\n",
    "                               \"p\": p,\n",
    "                               \"estimated_count\":int(np.round(np.power(10,mean))),\n",
    "                               })\n",
    "\n",
    "        # Obtaining Entropy of Time-Series values\n",
    "        prophetResultsDf = pd.DataFrame.from_records(prophet_results).sort_values('p')\n",
    "        prophetResultsDf['surprise'] = -np.log2(prophetResultsDf['p'])\n",
    "        return prophetResultsDf\n",
    "\n",
    "    \n",
    "    # Takes in a model that has been trained on country, plots graphs for visualization\n",
    "    def visualize(self):\n",
    "        '''\n",
    "        Builds plots for the forecast, displaying its construction on the basis of certain time intervals utilizing the \n",
    "        country fitted model self.countryModel\n",
    "        \n",
    "        Common issues: this step cannot be completed until you have trained the model on country (train_model_on_country)\n",
    "        and made a forecast (predict_future). Please complete these prior steps to build the forecast predictions prior to attempting\n",
    "        to visualize them.\n",
    "        '''\n",
    "        # Model visualization\n",
    "        fig = self.countryModel.plot(self.forecast)\n",
    "        fig = self.countryModel.plot_components(self.forecast)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPCA(Anomaly):\n",
    "    '''Combines Kernel Density and PCA into a join proccess that runs on all numerical columns to triangulate outliers'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(Anomaly):\n",
    "    \n",
    "    '''Column-based numerical outlier tester that utilizes fitting a Kernel and obtaining a density estimation'''\n",
    "    def __init__(self,x):\n",
    "        print(x)\n",
    "        \n",
    "    # Using cosine kernel function to get estimate for log density\n",
    "    def cosKernel(stat):\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'cosine').fit(stat)\n",
    "        cos_density = l.score_samples(stat)\n",
    "        return cos_density\n",
    "    \n",
    "    \n",
    "    # Using gaussian kernel function to get estimate for log density\n",
    "    def gaussKernel(stat):\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'gaussian').fit(stat)\n",
    "        density = l.score_samples(stat)\n",
    "        return density\n",
    "    \n",
    "    \n",
    "    # Using linear kernel function to get estimate for log density\n",
    "    def expKernel(stat):\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'exponential').fit(stat)\n",
    "        triDensity = l.score_samples(stat)\n",
    "        return triDensity\n",
    "    \n",
    "    \n",
    "    # Using epanechnikov kernel function to get estimate for log density\n",
    "    def parabolicKernel(stat):\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'epanechnikov').fit(stat)\n",
    "        epDensity = l.score_samples(stat)\n",
    "        return epDensity\n",
    "    \n",
    "    \n",
    "    # Specialized column based P-value function: double ended\n",
    "    def retPVal(col):\n",
    "        #Since we have a normal distribution, starting by obtaining the z-score\n",
    "        mean = col.mean()\n",
    "        std = np.std(col)\n",
    "        array = np.array([])\n",
    "        for i in np.arange(len(col)):\n",
    "            array = np.append(array, col.iloc[i] - mean)\n",
    "\n",
    "        #Now obtaining legitimate p-values\n",
    "        z_scores = array/std\n",
    "        for l in np.arange(len(z_scores)):\n",
    "            cdf = st.norm.cdf(z_scores[l])\n",
    "            z_scores[l] = min(cdf, 1-cdf)\n",
    "        return pd.Series(z_scores, index = col.index)\n",
    "    \n",
    "    \n",
    "    # Drops non-numerical and nan values from a table\n",
    "    def pcaPrep(first_table):   \n",
    "    # Finding all numerical components of the table so that pca can function\n",
    "        tabl = first_table.select_dtypes(include = [np.number])\n",
    "        tabl = tabl.dropna(1)\n",
    "        return tabl\n",
    "    \n",
    "    \n",
    "     #Assigning initial kernal estimations\n",
    "    def kernelEstimator(indx, stat):\n",
    "        kernelEstimate = pd.DataFrame()\n",
    "        kernelEstimate = kernelEstimate.assign(Data_Index = indx, Data_Point = stat,Gaussian = gaussKernel(stat),\n",
    "                                                   Epanechnikov = parabolicKernel(stat), Exponential = expKernel(stat),\n",
    "                                                   Cosine = cosKernel(stat))\n",
    "        # temporary sort for some visualization of surprise\n",
    "        kernelEstimate = kernelEstimate.sort_values(by = \"Gaussian\", ascending = False)\n",
    "        return kernelEstimate\n",
    "    \n",
    "    \n",
    "    # Calculating their average\n",
    "    def surprise_estimator(kernelEstimation):\n",
    "\n",
    "        # Calculating maximum number of deviations from the mean\n",
    "        numDevMax = (kernelEstimation.get(\"Data_Point\").max() - kernelEstimation.get(\"Data_Point\").mean())/kernelEstimation.get(\"Data_Point\").std()\n",
    "        numDevMin = (kernelEstimation.get(\"Data_Point\").min() - kernelEstimation.get(\"Data_Point\").mean())/kernelEstimation.get(\"Data_Point\").std()\n",
    "        numDev = max(numDevMax, numDevMin)\n",
    "\n",
    "        # Assigning appropriate Kernel Estimator\n",
    "        if(numDev > 3.2):\n",
    "            metric = retPVal(kernelEstimation.get(\"Exponential\"))\n",
    "        elif((numDev <=3.2) & (numDev >= 2)):\n",
    "            metric = retPVal(kernelEstimation.get(\"Gaussian\"))\n",
    "        else:\n",
    "            metric = retPVal(kernelEstimation.get(\"Exponential\")+kernelEstimation.get(\"Epanechnikov\"))  \n",
    "\n",
    "        # Surprise Metric\n",
    "        kernelEstimation  = kernelEstimation.assign(Surprise = -np.log2(metric))\n",
    "        kernelEstimation = kernelEstimation.sort_values(by = \"Surprise\", ascending = False)\n",
    "        return kernelEstimation\n",
    "    \n",
    "    \n",
    "    # A grouping of the entire kernel estimation process\n",
    "    def surprise_Table(Table, index = \"TEAM\"):\n",
    "        temp = pcaPrep(Table)\n",
    "\n",
    "        # Checking if index given\n",
    "        if(isinstance(index, str)):\n",
    "            index = Table.get(index)\n",
    "\n",
    "        #Obtaining surprise of every individual column\n",
    "        sum_surprise  = pd.Series(np.zeros(Table.shape[0]))\n",
    "        for col in temp.columns:\n",
    "            stat = temp.get(col)\n",
    "            KernelTable = kernelEstimator(index, stat)\n",
    "            KernelTable = surprise_estimator(KernelTable)\n",
    "            Table[col] = KernelTable.get(\"Surprise\")\n",
    "            sum_surprise+=Table[col]\n",
    "\n",
    "        # Averaging our surprise so we can sort by it\n",
    "        sum_surprise = sum_surprise.array\n",
    "        Table = Table.set_index(index)\n",
    "        Table = Table.assign(mean_surprise = np.round(sum_surprise/Table.shape[1],2))\n",
    "\n",
    "        # Sorting table for easier visualization\n",
    "        Table = Table.sort_values(by = \"mean_surprise\", ascending  = False)\n",
    "        return Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(Anomaly):\n",
    "    '''Row-based outlier techniques that utilizes dimensionality reduction to understand systematic bias by row'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical(Anomaly):\n",
    "    '''Uses dynamically built data \"grammar conventions\" to find outliers based on defiance of strict structures'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDimCategorical(Anomaly):\n",
    "    '''\n",
    "    Utilizes the idea of mutual entropy to build first order and 2nd order approximations for a \n",
    "    given column based on randomly chosen/handpicked context\n",
    "    '''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report(RunInitial):\n",
    "    '''An extention of the RunInitial class that offers a more verbose and visual report for an initial anomaly scan'''\n",
    "    data = None\n",
    "    hyperparams = None\n",
    "    \n",
    "    def __init__(self,x):\n",
    "        \n",
    "    def metadata(self):\n",
    "        pass\n",
    "    \n",
    "    def design_report(self):\n",
    "        pass\n",
    "        \n",
    "    def report_to_excel(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_pca(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_kernel_density(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_decision_tree(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
