{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for project purposes\n",
    "# Full Project imports\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import dateutil\n",
    "from datetime import datetime, timedelta\n",
    "import requests as rd\n",
    "import numpy as np\n",
    "from sklearn import neighbors, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import smtplib\n",
    "import scipy.stats as st\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision for each class:\n",
    "**Anomaly**: Base Class. User can load dataset, set train and test, and obtain a p-value: all the common utility functions. No other use.\n",
    "**Bootstrap**: Bootstrapping for category counts. Not to be included in report because too slow. User control = hyperparams. A tool.\n",
    "**TimeSeries**: TimeSeries anomaly estimation with visualization. Optional in initial report. User control = country. \n",
    "**Kernel-PCA**: Class used to combine Kernel Density and PCA. Will be run on initial DataSet (means alias needed in class). Returns a formatted table with marked anomaly. Builds off instances of both objects. Should extend Kernel Density class? User control = ???\n",
    "**Kernel**: Train-test set anomaly as well as choice of running on initial report. Works with Date/Time intervals as well though not sure how this will be incorporated into report. User Control = ???\n",
    "**PCA**: Report only anomaly? Does it make sense to even have train/test anomaly in this/Kernel Density??? I guess we are checking whether Kernel Model and PCA models for one fit well onto another, meaning inherent differences in shape: how does this apply to specific outliers - we are not testing Kernel Density/PCA as a model, but rather just using it as a means of changing distribution. \n",
    "**Categorical** - Returns nicely formatted DataFrame with Categorical outliers. Output also colored. User input = auto anomalous values. Multicategorical is an extension of Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anomaly:\n",
    "    \"\"\"Base Class for an anomaly detection method\"\"\"\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    \n",
    "    # Constructor to set values for data\n",
    "    def __init__(self, input_data = None):\n",
    "        \"\"\"\n",
    "        Constructor for setting dataset reference to a specific dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data: Pandas DataFrame reference - Your dataset in the form of a Pandas DataFrame\n",
    "        \"\"\"\n",
    "        # Ensuring data is properly formatted\n",
    "        assert input_data is None or type(input_data) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.data  = input_data\n",
    "        \n",
    "        \n",
    "    # Loading data into project\n",
    "    def load_html(self, link: str) -> pd.DataFrame():\n",
    "        \"\"\"\n",
    "        Loads an HTML table and sets it as the dataset for the model.\n",
    "        \n",
    "        Common issues: inputting an invalid file path (your file will not be read if this is the case),\n",
    "        linking another file format (ensure that your link is indeed a link to a website with tables), or giving a \n",
    "        link to a website which does not allow scraping of its information.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        \"\"\"\n",
    "        self.data = pd.read_html(link)\n",
    "        return self.data    \n",
    "    \n",
    "    \n",
    "    # Loading data into project\n",
    "    def load_csv(self, link: str) -> pd.DataFrame():\n",
    "        \"\"\"\n",
    "        Loads an CSV table and sets it as the dataset for the model. \n",
    "        \n",
    "        Common issues: Incorrect file path (ensure your file path is valid), a failure to enter a valid CSV\n",
    "        (ensure your file is in CSV format)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_excel(self, link: str) -> pd.DataFrame():\n",
    "        \"\"\"\n",
    "        Loads an Exel table and sets it as the dataset for the model.\n",
    "        \n",
    "        Common issues: Incorrect file path (ensure your file path is valid), a failure to enter a valid Excel\n",
    "        (ensure your file is in Excel format), Random spaces within your data (A random space within an Excel file might be read as an NaN value)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        \"\"\"\n",
    "        self.data = pd.read_excel(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_sql_table(sql: str, con, index_col: str = \"None\") -> pd.DataFrame():\n",
    "        \"\"\"\n",
    "        Loads a SQL table and sets it as the dataset for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        \"\"\"\n",
    "        self.data = pd.read_sql_table(sql, con, index_col)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Setter for the training set\n",
    "    def set_train(self, trainingSet: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        A setter for the training set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainingSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        \"\"\"\n",
    "        self.trainDf = trainingSet\n",
    "    \n",
    "    \n",
    "    # Setter for the test set\n",
    "    def set_test(self, testSet: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        A setter for the test set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        testSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        \"\"\"\n",
    "        self.testDf = testSet\n",
    "        \n",
    "    \n",
    "    # Randomly split train and test set\n",
    "    def assign_train_test(self,random_state = 42, training_set_ratio = 0.8, shuffling = True):\n",
    "        \"\"\"\n",
    "        A default random splitter into train and test set\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int - determines random state fed to model for reproducability of random results, default is 42\n",
    "        training_set_ratio: float between 0.0 and 1.0 - what % of your data you would like to encompass the training set (test set will be made in complimentary way) default is 0.8\n",
    "        shuffling: boolean - whether or not you would like your data randomly shuffled out of chronology prior to split (True/False). Default is True.\n",
    "        \"\"\"\n",
    "        # Ensuring that data actually exists before splitting\n",
    "        assert not(self.data is None), \"You cannot assign a train and test set out of a dataset that has not been initialized\"\n",
    "        \n",
    "        # Splitting into train and test\n",
    "        self.trainDf, self.testDf = train_test_split(self.data, train_size = training_set_ratio, shuffle = shuffling)\n",
    "        return self.trainDf, self.testDf\n",
    "    \n",
    "    \n",
    "    # Specialized column based P-value function: double ended\n",
    "    def return_p_value(self,col: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Takes a column a Z-score and plugs it into a cdf, returning back a Pandas Series of P-values\n",
    "        \n",
    "        Common issues: If Series does not have a set unique index, values will often become NaN's during computation.\n",
    "        Ensure your Series has an index prior to Series computations to avoid this issue.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        col: Pandas Series - A column to compute P-values on\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        P_value: Pandas Series\n",
    "                A Pandas Series containing the corresponding P-value (by index and in order) to every individual element\n",
    "                in the input column.\n",
    "        \"\"\"\n",
    "        # Ensuring only valid values are passed\n",
    "        assert not col.isnull().values.any(), \"return_p_value cannot calculate P values on an NaN element, please ensure your input does not contain elements of class np.nan\"\n",
    "        \n",
    "        #Since we have a normal distribution, starting by obtaining the z-score\n",
    "        mean = col.mean()\n",
    "        std = np.std(col)\n",
    "        centered = col.to_numpy()- mean\n",
    "        \n",
    "        #Now obtaining legitimate p-values\n",
    "        z_scores = centered/std\n",
    "        for l in np.arange(len(z_scores)):\n",
    "            cdf = st.norm.cdf(z_scores[l])\n",
    "            z_scores[l] = min(cdf, 1-cdf)\n",
    "        return pd.Series(z_scores, index = col.index)\n",
    "    \n",
    "    \n",
    " # Drops non-numerical and nan values from a table\n",
    "    def anomaly_prep(self, tabl: pd.DataFrame)-> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Removes non-numerical columns and NaN from the dataset for proccessing purposes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tabl: pandas DataFrame\n",
    "            A DataFrame containing the processed data   \n",
    "        \"\"\"\n",
    "        # Finding all numerical components of the table so that pca can function\n",
    "        tabl = self.data.select_dtypes(include = [np.number])\n",
    "        tabl = tabl.dropna(1)\n",
    "        return tabl\n",
    "       \n",
    "    \n",
    "    # Assigning colors to problematic values (still grouped with indices so easy to tell)\n",
    "    # Yellow: mild concern, Orange: serious concern, red - major concern\n",
    "    def designer(self, frame: pd.DataFrame = None):\n",
    "        \"\"\"\n",
    "        Takes a given DataFrame and Returns a version where every value exceeding a certain threshold will be colored \n",
    "        in a certain color to signify increasing levels of anomaly.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: Pandas DataFrame - A table to be colored\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        frame: Pandas DataFrame\n",
    "            The colored version of the table\n",
    "        \"\"\"\n",
    "        # Ensuring that we have a dataset to work with\n",
    "        if type(frame != pd.DataFrame):\n",
    "            frame = self.data\n",
    "            \n",
    "        assert type(frame) == pd.DataFrame, \"You must initialize a Dataset in order to use the class designer\"\n",
    "        \n",
    "        threshold1 = 5\n",
    "        threshold2 = 10\n",
    "        threshold3 = 15\n",
    "        print(\"Would you like to reset default issue alert thresholds?[5,10,15]\")\n",
    "        if(input().upper() == 'YES'):\n",
    "            print(\"Mild concern threshold (in probability (percentage) of issue being present):\")\n",
    "            threshold1 = float(input())\n",
    "            print(\"Moderate concern threshold (in probability (percentage) of issue being present)\")\n",
    "            threshold2 = float(input())\n",
    "            print(\"Serious concern threshold (in probability (percentage) of issue being present)\")\n",
    "            threshold3 = float(input())\n",
    "        temp = self.anomaly_prep(frame)\n",
    "        styler = frame.style\n",
    "        for col in temp.columns:\n",
    "            frame = styler.applymap(lambda x: 'background-color: %s' % 'yellow' if x > threshold1 else 'background-color: %s' % 'light-gray', subset = [col])\n",
    "            frame = styler.applymap(lambda x: 'background-color: %s' % 'orange' if x > threshold2 else 'background-color: %s' % 'light-gray', subset = [col])\n",
    "            frame = styler.applymap(lambda x: 'background-color: %s' % 'red' if x > threshold3 else 'background-color: %s' % 'light-gray', subset = [col])\n",
    "        return frame \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrap(Anomaly):\n",
    "    \"\"\"A class for returning anomaly of categorical column counts, utilizing the metric of surprise (entropy)\"\"\"\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    timestamp = None\n",
    "    params = None\n",
    "     \n",
    "    # Fot fitting data right away\n",
    "    def __init__(self, dataset: pd.DataFrame = None, timeCol: str = \"date_time\",  resamples: int = 1000, maxTrainingSizeMult: int = 10, maxCategory: int = 100, minCategories: int = 10):\n",
    "        \"\"\"\n",
    "        Overloaded constructor for attaching dataset immediately, can be done independently within any of the load functions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: String - A pandas data frame reference\n",
    "        timeCol: String - The name of the primary TimeStamp column. Default = \"date_time\".\n",
    "        resamples: int - the number of times the bootstrap resamples. Making this very large will improve accuracy but significantly lower speed. Default = 1000\n",
    "        maxTrainingSizeMult: int - If there is more than x  = maxTrainingSizeMult ratio of training to test data, trim training data to most recent. Default = 10\n",
    "        maxCategory: int - Maximum number of categories in a column (to ensure that counts are not tiny and are meaninful), column skipped if value count higher than this. Default = 100\n",
    "        minCategory: int - if column has a category count that is lower than this value, don't report it in bootstrap surprise. Default = 10.\n",
    "        \"\"\"\n",
    "        self.timestamp = timeCol\n",
    "        self.data = dataset\n",
    "        \n",
    "        # Meta-parameter initialization\n",
    "        self.params = {\n",
    "          \"bootstrapResamples\": resamples,\n",
    "          \"maxTrainingSizeMultiple\":maxTrainingSizeMult, # if there is more than X times more training data, trim to most recent\n",
    "          \"maxCategories\":maxCategory,\n",
    "          \"minCategoryCount\": minCategories,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Converts Timetamp column of DataFrame to a legitimate timestamp\n",
    "    def convert_timestamp_to_datetime(self, formatting: str = '%Y%m%d %H:%M:%S') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a chosen timestamp column from string to date/time, making the modifications both to the fitted\n",
    "        Data Frame and returning the new Data Frame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp: String - The name of the Timestamp column that needs conversion\n",
    "        formatting: String - If formatting different from default = %Y%m%d %H:%M:%S, enter the format of your TimeSeries column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the locally the entire DataFrame with the modified Timestamp column\n",
    "        \"\"\"\n",
    "        self.data[self.timestamp] =  pd.to_datetime(self.data[self.timestamp], format = formatting)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(self, batchHours: int = 24*7):\n",
    "        \"\"\"\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        \"\"\"\n",
    "        data_type = type(self.data[self.timestamp].iloc[0])\n",
    "        assert  data_type != str, \"Timestamp Column should be converted to datetime via convert_timestamp_to_datetime prior to split\" \n",
    "        \n",
    "        # Splitting by interval = batchHours\n",
    "        maxTs = max(self.data[self.timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[self.timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[self.timestamp] < batchTs]\n",
    "        \n",
    " \n",
    "    # Helpers and Math\n",
    "    def p_value(self,data, threshold: np.number, result: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Returns the p-value of a computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Pandas DataFrame - The Data we are computing the P-value on\n",
    "        threshold: np.number - The threshold to check if data is anomalous\n",
    "        result: pd.DataFrame - A DataFrame containing the column \"Bootstrap counts\" to be normalized and tested for anomaly\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pGauss : np.array\n",
    "            Returns the array of normalized p-values for each bootstrap count\n",
    "        \"\"\"\n",
    "        # Taking the smaller of the 2 p-values(either could present large anomaly)\n",
    "        pLarger = sum(np.array(data) >= threshold) / len(data)\n",
    "        pSmaller = sum(np.array(data) <= threshold) / len(data)\n",
    "        p = min(pLarger, pSmaller)\n",
    "\n",
    "        # only use gaussian p-value when there is variation, but bootsrap p = 0\n",
    "        stdev = np.std(data)\n",
    "        if stdev == 0 or p != 0:\n",
    "            pGauss = p\n",
    "        else:\n",
    "            # Normalizing\n",
    "            pGauss = st.norm(np.mean(result['bootstrap_counts']), stdev).cdf(result['count'])\n",
    "            pGauss = min(pGauss,1-pGauss)\n",
    "        return pGauss\n",
    "\n",
    "    \n",
    "    def trim_table(self,df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trims a DataFrame, ensuring that it does not exceed the training set max size hyper parameter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame that is being trimmed to fit to the training set hyperparameter\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dfTrimmed : pandas DataFrame\n",
    "            Returns a DataFrame fit to the training set specifications\n",
    "        \"\"\"\n",
    "        # trim to most recent\n",
    "        df = df.sort_values(self.timestamp, ascending =False)\n",
    "        dfTrimmed = df[:self.params['maxTrainingSizeMultiple']*len(self.testDf)]\n",
    "\n",
    "        return dfTrimmed\n",
    "    \n",
    "    \n",
    "    # Returns names of categorical columns\n",
    "    def get_categorical_col_names(self, df: pd.DataFrame) -> []:\n",
    "        \"\"\"\n",
    "        Returns the names of categorical columns in a Pandas DataFrame (if the type is a string)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame whose columns are checked for being categorical data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        columnNames : list\n",
    "            The list of all categorical column names \n",
    "        \"\"\"\n",
    "        columnNames = []\n",
    "        for columnName in df.keys():\n",
    "            if (type (df[columnName].iloc[0])) == str:\n",
    "                columnNames.append(columnName)\n",
    "        return columnNames\n",
    "    \n",
    "    \n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Tests for difference between training and test set counts, returning a report that quantifies difference between\n",
    "        training and test set as surprise.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        resultsDf : pandas DataFrame\n",
    "            A DataFrame containing a report for the difference between expected and detected counts within the test set \n",
    "            With the inclusion of a column quantifying irregularity as surprise (entropy)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Preventative measures\n",
    "        assert type(self.trainDf) == pd.DataFrame and type(self.testDf) == pd.DataFrame, \"Please set up your train and test sets as Pandas DataFrames prior to attempting this step\"\n",
    "        \n",
    "        # get all of the string columns\n",
    "        columnNames = self.get_categorical_col_names(self.testDf)\n",
    "\n",
    "        bootstrapDf = self.trim_table(self.trainDf)\n",
    "\n",
    "        # set up dict, add counts\n",
    "        results = {}\n",
    "\n",
    "\n",
    "        for columnName in columnNames:\n",
    "\n",
    "            # if it isn't a string column, reject it\n",
    "            if type(self.testDf[columnName].iloc[0]) != str:\n",
    "                continue\n",
    "            categories = (bootstrapDf[columnName].append(self.testDf[columnName])).unique()\n",
    "            if len(categories) > self.params['maxCategories']:\n",
    "                continue\n",
    "\n",
    "            results[columnName] = {}\n",
    "            testCounts = self.testDf[columnName].value_counts(dropna = False)\n",
    "            \n",
    "            \n",
    "            for i in np.arange(1,len(categories) -1):\n",
    "                if(pd.isna(categories[i])):\n",
    "                    categories = np.delete(categories, i)  \n",
    "            for category in categories:\n",
    "                results[columnName][category] = {'bootstrap_counts':[],\n",
    "\n",
    "                                                 'count':testCounts.get(category,0)}\n",
    "        # resample, add boostrap counts\n",
    "        for ii in range(self.params['bootstrapResamples']):\n",
    "\n",
    "            # Draw random sample from training\n",
    "            sampleDf = bootstrapDf.sample(len(self.testDf), replace=True)\n",
    "            for columnName in results.keys():\n",
    "\n",
    "                # count by category\n",
    "                trainCounts = sampleDf[columnName].value_counts(dropna = False)\n",
    "\n",
    "                # put results in dict\n",
    "                for category in results[columnName].keys():\n",
    "                    boostrapCount = trainCounts.get(category,0)\n",
    "                    results[columnName][category]['bootstrap_counts'].append(boostrapCount)\n",
    "\n",
    "        # convert to records, add p-values\n",
    "        bootstrap_results = []\n",
    "        for columnName in results.keys():\n",
    "            for category in results[columnName].keys():\n",
    "                result = results[columnName][category]\n",
    "\n",
    "                estimatedCount = int(np.round(np.mean(result['bootstrap_counts'])))\n",
    "\n",
    "                # don't report entries with very low predicted and actual counts\n",
    "                if estimatedCount < self.params['minCategoryCount'] and result['count'] < self.params['minCategoryCount']:\n",
    "                    continue\n",
    "\n",
    "                p = self.p_value(result['bootstrap_counts'],result['count'], result)\n",
    "                categoryName = category\n",
    "\n",
    "                # Backup\n",
    "                if not category:\n",
    "                    categoryName = \"NULL\"\n",
    "\n",
    "                bootstrap_results.append({\"column\":columnName,\n",
    "                                   \"category\":categoryName,\n",
    "                                   \"count\":result['count'],\n",
    "                                   \"p\": p,\n",
    "                                   \"estimated_count\":estimatedCount,\n",
    "                                   })\n",
    "\n",
    "        # Sorting by P-values and obtaining Surprise of each\n",
    "        if(np.count_nonzero(p)>0):\n",
    "            resultsDf = pd.DataFrame.from_records(bootstrap_results).sort_values('p')\n",
    "            resultsDf['surprise'] = -np.log2(resultsDf['p'])\n",
    "\n",
    "            return resultsDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:fbprophet:Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "class TimeSeries(Anomaly):\n",
    "    \"\"\"\n",
    "    Utilizes facebook prophet and its ability to predict the future based off specific time context (day, hour, holiday)\n",
    "    to make predictions and test those against the dataset, thus finding anomaly with the context of time. Please ensure \n",
    "    you set your train and test sets prior to computation.\n",
    "    \"\"\"\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    prophetDf = None\n",
    "    countryModel = None\n",
    "    forecast = None\n",
    "    timestamp = None\n",
    "    \n",
    "    def __init__(self,timeStampInput: str = 'date_time', inp_data = None, train = None,test = None):\n",
    "        \"\"\"\n",
    "        Does not require any inputs, but gives user option to initialize input data/train/test right from the get-go\n",
    "        \n",
    "        Common Issues: Please ensure you set the name of your timestamp column. Default value  = \"date_time\". Not setting the name\n",
    "        of this column will result in an error.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_data: pandas DataFrame - A given data set\n",
    "        train: pandas DataFrame - A given training set\n",
    "        test: pandas DataFrame - A given test set\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensuring that if user has given us input, it is of the correct form\n",
    "        assert inp_data is None or type(inp_data) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.data  = inp_data\n",
    "        \n",
    "        # Ensuring training set is of the correct form\n",
    "        assert train is None or type(train) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.trainDf  = train\n",
    "        \n",
    "        # Ensuring test set is of the correct form\n",
    "        assert test is None or type(test) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.testDf = test\n",
    "        \n",
    "        \n",
    "        self.timestamp = timeStampInput\n",
    "     \n",
    "    \n",
    "    # Converts Timetamp column of DataFrame to a legitimate timestamp\n",
    "    def convert_timestamp_to_datetime(self, formatting: str = '%Y%m%d %H:%M:%S') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts a chosen timestamp column from string to date/time, making the modifications both to the fitted\n",
    "        Data Frame and returning the new Data Frame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp: String - The name of the Timestamp column that needs conversion\n",
    "        formatting: String - If formatting different from default = %Y%m%d %H:%M:%S, enter the format of your TimeSeries column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the locally the entire DataFrame with the modified Timestamp column\n",
    "        \"\"\"\n",
    "        self.data[self.timestamp] =  pd.to_datetime(self.data[self.timestamp], format = formatting)\n",
    "        return self.data\n",
    "    \n",
    "            \n",
    "    def truncate_timestamp_column(self, ts) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Truncates a timestamp column to the hour percision (minute, second, and microsecond all set to 0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ts: pandas Series - A given Timestamp column to be truncated\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ts : pandas Series\n",
    "            The Timestamp column truncated to the hour percision\n",
    "        \"\"\"\n",
    "        return ts.dt.floor(\"H\")\n",
    "    \n",
    "    \n",
    "    def group_and_build_time_table(self,truncated: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Builds a table on the basis of the value counts (or rather the log 10 of the value counts)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        truncated: boolean - A value representing whether or not the training set has already been truncated to the hour. Default is False. If this is the case, the data will be autotruncated. \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prophetDf : pandas Data Frame\n",
    "                   A table with value grouping by value counts\n",
    "        \n",
    "        \"\"\"\n",
    "        # Making a copy as to not mess up reference\n",
    "        truncatedData  = self.trainDf.copy()\n",
    "        \n",
    "        # Truncating timestamp if needed\n",
    "        if truncated == False:\n",
    "            truncatedData = self.truncate_timestamp_column(truncatedData[self.timestamp])\n",
    "        else:\n",
    "            truncatedData = truncatedData[self.timestamp]\n",
    "        groupedCounts = truncatedData.value_counts()\n",
    "        \n",
    "        # Grouping counts in a single DataFrame\n",
    "        self.prophetDf = pd.DataFrame({'ds':groupedCounts.index,'y':np.log10(groupedCounts.values)})\n",
    "        return self.prophetDf\n",
    "    \n",
    "    \n",
    "    # Takes in the the dataset and the prophet dataset returned by the ast option\n",
    "    def train_model_on_country(self, country: str = \"US\"):\n",
    "        \"\"\"\n",
    "        Trains a Facebook Prophet model on a specified country with a linear growth algorithm and an interval\n",
    "        width of one sigma. Default country is the United States. Will fit this country model onto TimeSeries table.\n",
    "        \n",
    "        Common issues: Downloading Prophet can be very messy and certain modifications might need to be made to \n",
    "        enable holidays such as Easter. You can read more about this issue here: \n",
    "        \n",
    "        https://github.com/facebook/prophet/issues/1293\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Country: String - The name of a valid country included in the Prophet seasonality package\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.countryModel: Table\n",
    "                          Facebook Prophet model fitted onto the country of your choice (is also now contained as\n",
    "                          an instance variable)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Ensuring inputted country is a string\n",
    "        assert type(country) == str, \"Given country should be formatted as a string\"\n",
    "        # Train model\n",
    "        self.countryModel = Prophet(#daily_seasonality = True, \n",
    "                    #yearly_seasonality = False, \n",
    "                    #weekly_seasonality = True, \n",
    "                    #growth='linear',\n",
    "                    interval_width=0.68 # one sigma\n",
    "                   )\n",
    "        self.countryModel.add_country_holidays(country_name=country)\n",
    "\n",
    "        self.countryModel.fit(self.prophetDf)\n",
    "        return self.countryModel\n",
    "    \n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(self,batchHours: int = 24*7):\n",
    "        \"\"\"\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        \"\"\"\n",
    "        maxTs = max(self.data[self.timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[self.timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[self.timestamp] < batchTs]\n",
    "        \n",
    "\n",
    "    # Applies Prophet analytics to create a forecast based on hours\n",
    "    def predict_future(self, timestamp: str = \"date_time\"):\n",
    "        \"\"\"\n",
    "        Builds (and returns) a future forecast for comparison to test set (which should be further ahead in time relative \n",
    "        to the trainig set). Made on the basis of the number of hours which encompass the test set.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.forecast: Table\n",
    "                      A forecast representiative of the predictions the Prophet Model would assume the test set to be,\n",
    "                      based on the training set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Takes in trained model and predicts the future\n",
    "        # find number of hours to preduct: ceil of hours in testDf\n",
    "        \n",
    "        # Obtaining interval contained by test set for computation purposes.\n",
    "        timeDelta = max(self.testDf[timestamp]) - min(self.testDf[timestamp])\n",
    "\n",
    "        #If a column is string, convert to date/time\n",
    "        if self.testDf.applymap(type).eq(str).any()[timestamp]:\n",
    "            self.testDf[self.timestamp] = pd.to_datetime(self.testDf[self.timestamp])\n",
    "\n",
    "        timeDelta = max(self.testDf[self.timestamp]) -min(self.testDf[self.timestamp])\n",
    "        hours = int(timeDelta.days*24 + timeDelta.seconds/(60*60))+1\n",
    "        future = self.countryModel.make_future_dataframe(periods = hours, freq = 'H')\n",
    "        self.forecast = self.countryModel.predict(future)\n",
    "        return self.forecast\n",
    "\n",
    "    def train_test_anomaly(self, country = \"US\"):\n",
    "        \"\"\"\n",
    "        Fits Prophet Model on Train set and runs predictions on test set, finding the difference between expected counts and \n",
    "        TimeSeries predicted counts while also accounting for the anomaly in the difference between the 2 estimations.\n",
    "\n",
    "        Common Issues: Ensure your timestamp has been set as the correct value prior to run.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        Country: String - the name of the country whose holidays and timezone you would like to train the Prophet Time Series model on\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prophetResultsDf: Pandas DataFrame\n",
    "                      A table containing difference between observed and expected counts, sorted in order of the metric of surprise\n",
    "                      (How anomalous/chaotic/entropic the data is)\n",
    "        \"\"\"\n",
    "        # If train and test sets have not been split prior to run, split them\n",
    "        if(type(self.train_test_anomaly != pd.DataFrame)):\n",
    "            self.split_train_test_by_time()\n",
    "\n",
    "        # Training the model on country\n",
    "        print(\"Train and Test Sets Prepared\\n\")\n",
    "        tms.group_and_build_time_table(False)\n",
    "        tms.train_model_on_country(country)\n",
    "        print(\"Model trained...\\n\")\n",
    "\n",
    "        #  Predicting values\n",
    "        tms.predict_future(self.timestamp)\n",
    "        print(\"Future predicted...\\n\")\n",
    "\n",
    "        # Returning anomaly estimation\n",
    "        return tms.surprise_estimator(country)\n",
    "\n",
    "        \n",
    "    def surprise_estimator(self, country: str = \"US\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Based on the training-set-reliant prediction of the future, calculates the anomaly between the training and the\n",
    "        test set utilizing the metric of surprise.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        Country: String - the name of the country whose holidays and timezone you would like to train the Prophet Time Series model on\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        prophetResultsDf: Pandas DataFrame\n",
    "                      A table containing difference between observed and expected counts, sorted in order of the metric of surprise\n",
    "                      (How anomalous/chaotic/entropic the data is)\n",
    "        \"\"\"\n",
    "        # Processing training set and making predictions\n",
    "        truncated = self.truncate_timestamp_column(self.trainDf[self.timestamp])\n",
    "        groupedCounts = truncated.value_counts()\n",
    "\n",
    "        prophetTestDf = pd.DataFrame({'ds':groupedCounts.index,\n",
    "                                      'y':np.log10(groupedCounts.values),\n",
    "                                      'y_linear':groupedCounts.values})\n",
    "\n",
    "        # find p-value\n",
    "        prophet_results = []\n",
    "\n",
    "        # Comparing test and training set data for identical intervals\n",
    "        for ii in range(len(prophetTestDf)):\n",
    "            ts = prophetTestDf['ds'][ii]\n",
    "            fcstExample = self.forecast[self.forecast['ds'] == ts]\n",
    "            mean = fcstExample['yhat'].iloc[0]\n",
    "            stdev = (fcstExample['yhat_upper'].iloc[0] - fcstExample['yhat_lower'].iloc[0])/2\n",
    "\n",
    "            # Calculating the P-value\n",
    "            p = st.norm(mean, stdev).cdf(prophetTestDf['y'][ii])\n",
    "            p = min(p,1-p)\n",
    "\n",
    "            prophet_results.append({\"column\":\"Forecast\",\n",
    "                               \"category\":str(ts),\n",
    "                               \"count\":prophetTestDf['y_linear'][ii],\n",
    "                               \"p\": p,\n",
    "                               \"estimated_count\":int(np.round(np.power(10,mean))),\n",
    "                               })\n",
    "\n",
    "        # Obtaining Entropy of Time-Series values\n",
    "        prophetResultsDf = pd.DataFrame.from_records(prophet_results).sort_values('p')\n",
    "        prophetResultsDf['surprise'] = -np.log2(prophetResultsDf['p'])\n",
    "        return prophetResultsDf\n",
    "\n",
    "    \n",
    "    # Takes in a model that has been trained on country, plots graphs for visualization\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Builds plots for the forecast, displaying its construction on the basis of certain time intervals utilizing the \n",
    "        country fitted model self.countryModel\n",
    "        \n",
    "        Common issues: this step cannot be completed until you have trained the model on country (train_model_on_country)\n",
    "        and made a forecast (predict_future). Please complete these prior steps to build the forecast predictions prior to attempting\n",
    "        to visualize them.\n",
    "        \"\"\"\n",
    "        # Model visualization\n",
    "        fig = self.countryModel.plot(self.forecast)\n",
    "        fig = self.countryModel.plot_components(self.forecast)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPCA(Anomaly):\n",
    "    \"\"\"Combines Kernel Density and PCA into a join proccess that runs on all numerical columns to triangulate outliers\"\"\"\n",
    "    data = None\n",
    "    knal = None\n",
    "    principalComp = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    replace_nans_with = None\n",
    "    \n",
    "    def __init__(self,dataInit: pd.DataFrame = None, trainInit: pd.DataFrame = None, testInit: pd.DataFrame = None, replace_nans_with = False):\n",
    "        \"\"\"\n",
    "        Gives user the option to initialize Data Set, Training Set, and Test Set\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: pandas DataFrame - A given data set\n",
    "        train: pandas DataFrame - A given training set\n",
    "        test: pandas DataFrame - A given test set\n",
    "        \"\"\"\n",
    "        # Initializing datasets prior to build with neccessary information\n",
    "        self.data = dataInit\n",
    "        self.trainDf = trainInit\n",
    "        self.testDf = testInit\n",
    "        self.replace_nans_with = replace_nans_with\n",
    "        \n",
    "        \n",
    "    # Builds Kernel and PCA models based on input parameters\n",
    "    def build(self) -> str:\n",
    "        \"\"\"\n",
    "        Builds Kernel and PCA objects for run on dataset\n",
    "        \n",
    "        Common Issues: You must either load your dataset or set up both train and test set prior to running this method. You can do this \n",
    "        using the methods in the Anomaly() class (which this class extends)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"Build Successful\" : str\n",
    "                         Indication that the objects have been built successfully\n",
    "        \"\"\"\n",
    "        assert type(self.data) == pd.DataFrame or (type(train) == pd.DataFrame and type(test)== pd.DataFrame), \"Please set up your dataset or your train and test sets prior to running build()\"\n",
    "        self.knal = Kernel(self.data,self.trainDf, self.testDf, replace_nan_with = self.replace_nans_with)\n",
    "        self.principalComp = PCA_Analysis(self.data, self.trainDf, self.testDf, replace_nan_with = self.replace_nans_with)\n",
    "        return \"Build Successful\"\n",
    "        \n",
    "    \n",
    "    # Runs a Data Quality Test on Current DataSet: for Report\n",
    "    def run_on_current_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Runs the Kernel Density- Principal Component Analysis anomaly test Combo on current dataset, returning the surprise metric for every element in a DataFrame\n",
    "        \n",
    "        Common Issues: An index must be provided to group the data, and can only be run after build() has been run and dataset has\n",
    "        been initialized.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        index: String - An index to use in building Kernel Density and Principal Component Analysis Models\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        final_anomaly: Pandas DataFrame\n",
    "                      A DataFrame with an anomaly approximation for each value in the dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Finding respective Kernel and PCA anomalies\n",
    "        knal_anomaly = self.knal.surprise_of_table()\n",
    "        pca_anomaly = self.principalComp.surprise_on_table()\n",
    "        \n",
    "        # Obtaining PCA Surprise factor\n",
    "        pcaSurpriseCol = pca_anomaly.get(\"Surprise\")\n",
    "        temp = self.principalComp.pca_prep(knal_anomaly)\n",
    "        \n",
    "        # Traversing and Multiplying anomaly\n",
    "        for column in temp.columns:\n",
    "            knal_anomaly[column] = (knal_anomaly[column].multiply(pcaSurpriseCol)).apply(np.sqrt)\n",
    "            \n",
    "        # Sorting by Surprise of values\n",
    "        final_anomaly = knal_anomaly.sort_values(by = \"mean_surprise\", ascending = False)\n",
    "        return final_anomaly\n",
    "\n",
    "    \n",
    "    \n",
    "    # Calculates Anomaly between Train and Test\n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Runs the Kernel Density- Principal Component Analysis anomaly test Combo on train and test sets, returning the surprise metric for every test-set element in a DataFrame\n",
    "        \n",
    "        Common Issues: Can only be run after build() has been run and train + test sets have been initialized.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        final_anomaly: Pandas DataFrame\n",
    "                      A DataFrame with an anomaly approximation for each value in the test set\n",
    "        \"\"\"\n",
    "        knal_anomaly = self.knal.train_test_anomaly()\n",
    "        pca_anomaly = self.principalComp.train_test_anomaly()\n",
    "        \n",
    "        # Obtaining PCA Surprise factor\n",
    "        pcaSurpriseCol = pca_anomaly.get(\"Surprise\")\n",
    "        temp = self.principalComp.pca_prep(knal_anomaly)\n",
    "        mean_surp = np.zeros(temp.shape[0])\n",
    "        \n",
    "        # Traversing and Multiplying anomaly\n",
    "        for column in temp.columns:\n",
    "            knal_anomaly[column] = (knal_anomaly[column].multiply(pcaSurpriseCol)).apply(np.sqrt)\n",
    "            mean_surp += knal_anomaly[column].values\n",
    "            \n",
    "        # Adding in mean_surprise\n",
    "        knal_anomaly = knal_anomaly.assign(mean_surprise = mean_surp / knal_anomaly.shape[1])\n",
    "        \n",
    "            \n",
    "        # Sorting by Surprise of values\n",
    "        final_anomaly = knal_anomaly.sort_values(by = \"mean_surprise\", ascending = False)\n",
    "        return final_anomaly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(Anomaly):\n",
    "    \"\"\"Column-based numerical outlier tester that utilizes fitting a Kernel and obtaining a density estimation\"\"\"\n",
    "    data  = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    timestamp_anom = None\n",
    "    timestamp = None\n",
    "    replace_nan = None\n",
    "      \n",
    "    def __init__(self, data_value:pd.DataFrame = None, train_set: pd.DataFrame = None, test_set: pd.DataFrame = None, timestamp_anom: bool = False,timestamp_init = 'date_time', replace_nan_with = False):\n",
    "        \"\"\"\n",
    "        Object constructor which allows user to set dataset as default and initialize train+test sets. \n",
    "        Keep in mind the dataset can also be loaded by means of any method in the base Anomaly class.\n",
    "        You also don't have to initialize your train and test sets right away and can use our random splitter to do so prior to running the Kernel Density itself!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_value: Pandas DataFrame - The dataset you would like to fit Kernel Density to\n",
    "        train_set: Pandas DataFrame  - The train set you would like to use for Kernel Density purposes (default = None)\n",
    "        test_set: Pandas DataFrame  - The test set you would like to use for Kernel Density purposes (default = None)\n",
    "        timestamp_anom: boolean - A boolean variable indicating whether or not user wants to run Kernel Density on a date/time column (Default = False)\n",
    "        timestamp: String - The name of the timestamp column (default = 'date_time')\n",
    "        \"\"\"\n",
    "        self.data = data_value\n",
    "        self.trainDf = train_set\n",
    "        self.testDf = test_set\n",
    "        self.timestamp_anom = timestamp_anom\n",
    "        \n",
    "        # Ensuring that user wants anomaly\n",
    "        if(self.timestamp_anom == True):\n",
    "            self.timestamp = timestamp_init\n",
    "        \n",
    "        # Warning beginning user if train-test balance is off\n",
    "        if type(self.trainDf) == pd.DataFrame and type(self.testDf) == pd.DataFrame and self.trainDf.shape[0] < self.testDf.shape[0]:\n",
    "            print('Warning: training set larger than test set. Could potentially damage results')\n",
    "        \n",
    "        self.replace_nan = replace_nan_with\n",
    "        \n",
    "        \n",
    "    # Using cosine kernel function to get estimate for log density\n",
    "    def cosine_kernel(self, stat: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits a Cosine Kernel to the data and scores samples by their density.\n",
    "        Used for distributions with low variability.\n",
    "        \n",
    "        Common issues: argument stat should be a Pandas Series: not a DataFrame or an array. Should contain numerical values only,\n",
    "        NaN's and non numerical values will break the cosine_kernel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas DataFrame - A Pandas Series you would like to fit the cosine Kernel to\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cos_density: np.ndarray\n",
    "                  A 2 dimensional array with all the scored log densities. Can read more here:\n",
    "                  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensuring that there are no NaN values that may break the Kernel process\n",
    "        assert not stat.isnull().values.any(), \"Please make sure to preprocess your table prior so that Nan's and non-numerical values are removed, you can do so with the Kernel.kernel_prep() method\"\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'cosine').fit(stat)\n",
    "        cos_density = l.score_samples(stat)\n",
    "        return cos_density\n",
    "    \n",
    "    \n",
    "    # Using gaussian kernel function to get estimate for log density\n",
    "    def gaussian_kernel(self,stat: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits a Gaussian Kernel to the data and scores samples by their density.\n",
    "        Used for distributions with standard Gaussian variability.\n",
    "        \n",
    "        Common issues: argument stat should be a Pandas Series: not a DataFrame or an array. Should contain numerical values only,\n",
    "        NaN's and non numerical values will break the cosine_kernel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas DataFrame - A Pandas Series you would like to fit the cosine Kernel to\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        density: np.ndarray\n",
    "                  A 2 dimensional array with all the scored log densities. Can read more here:\n",
    "                  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html\n",
    "        \"\"\"\n",
    "        assert not stat.isnull().values.any(), \"Please make sure to preprocess your table prior so that Nan's and non-numerical values are removed, you can do so with the Kernel.kernel_prep() method\"\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'gaussian').fit(stat)\n",
    "        density = l.score_samples(stat)\n",
    "        return l, density\n",
    "    \n",
    "    \n",
    "    # Using linear kernel function to get estimate for log density\n",
    "    def exponential_kernel(self,stat: pd.Series)-> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits an Exponential (fatty tailed) Kernel to the data and scores samples by their density.\n",
    "        Used for distributions with high variability.\n",
    "        \n",
    "        Common issues: argument stat should be a Pandas Series: not a DataFrame or an array. Should contain numerical values only,\n",
    "        NaN's and non numerical values will break the cosine_kernel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas DataFrame - A Pandas Series you would like to fit the cosine Kernel to\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        density: np.ndarray\n",
    "                  A 2 dimensional array with all the scored log densities. Can read more here:\n",
    "                  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html\n",
    "        \"\"\"\n",
    "        assert not stat.isnull().values.any(), \"Please make sure to preprocess your table prior so that Nan's and non-numerical values are removed, you can do so with the Kernel.kernel_prep() method\"\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'exponential').fit(stat)\n",
    "        expDensity = l.score_samples(stat)\n",
    "        return l, expDensity\n",
    "    \n",
    "    \n",
    "    # Using epanechnikov kernel function to get estimate for log density\n",
    "    def parabolic_kernel(self,stat: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fits a Parabolic/Epenechnikov Kernel to the data and scores samples by their density. \n",
    "        Used for distributions with low variability\n",
    "        \n",
    "        Common issues: argument stat should be a Pandas Series: not a DataFrame or an array. Should contain numerical values only,\n",
    "        NaN's and non numerical values will break the cosine_kernel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas DataFrame - A Pandas Series you would like to fit the cosine Kernel to\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        epDensity: np.ndarray\n",
    "                  A 2 dimensional array with all the scored log densities. Can read more here:\n",
    "                  https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html\n",
    "        \"\"\"\n",
    "        assert not stat.isnull().values.any(), \"Please make sure to preprocess your table prior so that Nan's and non-numerical values are removed, you can do so with the Kernel.kernel_prep() method\"\n",
    "        stat = stat.to_numpy().reshape(-1,1) \n",
    "        l = neighbors.KernelDensity(kernel = 'epanechnikov').fit(stat)\n",
    "        epDensity = l.score_samples(stat)\n",
    "        return l, epDensity\n",
    "    \n",
    "\n",
    "    # Drops non-numerical and nan values from a table\n",
    "    def kernel_prep(self, data = None)-> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Removes non-numerical columns and NaN from the dataset for proccessing purposes\n",
    "        \n",
    "        Common Issues: Ensure that the either the data value is passed or your class dataset is initialized.\n",
    "        Failure of either will result in an error.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tabl: pandas DataFrame\n",
    "            A DataFrame containing the processed data   \n",
    "        \"\"\"\n",
    "        # If dataset not given, assume it is our preinitialized dataset\n",
    "        if(type(data)!= pd.DataFrame):\n",
    "            data = self.data\n",
    "            \n",
    "        # Finding all numerical components of the table so that pca can function\n",
    "        tabl = data.select_dtypes(include = [np.number])\n",
    "        if type(self.replace_nan) != bool:\n",
    "            self.replace_nan = float(self.replace_nan)\n",
    "            tabl.replace(to_replace = [np.nan], value =[self.replace_nan], inplace=True)\n",
    "        else:\n",
    "            tabl = tabl.dropna(1)\n",
    "        return tabl\n",
    "    \n",
    "    \n",
    "    # Fits proper Kernel and returns the surprise by element\n",
    "    def surprise_estimator(self,stat: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Returns the surprise on a per-value basis of the Kernel density estimation for a particular column.\n",
    "        \n",
    "        Common issues: Series should have a set index, one that generalizes to the rest of the dataset. Not having\n",
    "        such an index will cause trouble when grouping all columns into a single DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas Series - A Series of numerical values with a set index.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Surprise: Pandas Series\n",
    "            A Series containing each value index and its corresponding suprise.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Surprise Metric\n",
    "        model, metrics = self.kernel_decider(stat)\n",
    "        pVals = self.return_p_value(pd.Series(metrics))\n",
    "        surprise = -np.log2(pVals)\n",
    "        return surprise.sort_values()\n",
    "    \n",
    "    \n",
    "    def kernel_decider(self,stat: pd.Series)-> pd.Series:\n",
    "        \"\"\"\n",
    "        Returns Kernel density estimations for a particular column, decided based off the column's variability.\n",
    "        \n",
    "        Common issues: Series should have a set index, one that generalizes to the rest of the dataset. Not having\n",
    "        such an index will cause trouble when grouping all columns into a single DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stat: Pandas Series - A Series of numerical values with a set index.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        metric: Pandas Series\n",
    "            A Series containing each value index and its corresponding Kernel Density Estimation.\n",
    "        \"\"\"\n",
    "        # Calculating maximum number of deviations from the mean so as to choose proper Kernel model\n",
    "        mean = stat.mean()\n",
    "        dev = stat.std()\n",
    "        numDevMax = (stat.max() - mean)/dev\n",
    "        numDevMin = (stat.min() - mean)/dev\n",
    "        numDev = max(numDevMax, numDevMin)\n",
    "        \n",
    "        metric = None\n",
    "        model = None\n",
    "        \n",
    "        # Assigning appropriate Kernel Estimator on the basis of model's variability\n",
    "        if(numDev > 3.2):\n",
    "            model,metric = self.exponential_kernel(stat)\n",
    "        elif((numDev <=3.2) & (numDev >= 2)):\n",
    "            model, metric = self.gaussian_kernel(stat)\n",
    "        else:\n",
    "            model, metric = self.parabolic_kernel(stat)\n",
    "            \n",
    "        return model, metric\n",
    "\n",
    "    \n",
    "    # Calculation of date time entropies\n",
    "    def date_time_interval_anomaly(self,column: pd.Series)-> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the anomaly for the intervals between the timestamps, in order of anomaly (surprise), using ordinary data index.\n",
    "        \n",
    "        Common issues: Given column should be a timestamp in either string or datetime format. Integer or floating point timestamps\n",
    "        will cause issues. Timestamps before 1970 (Unix Threshold) will also cause issues.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - An Series representing date/time values in a DataFrame.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dateSurprise: Pandas DataFrame\n",
    "            A DataFrame consisting of the table index, the interval, and the corresponding surprise.\n",
    "        \"\"\"\n",
    "        # Conversion to proper format\n",
    "        if (type(column.iloc[0]) == str):\n",
    "            column = self.convert_to_date_time(column)\n",
    "\n",
    "        # Unix timestamps for ease of calculation\n",
    "        unixCol = column.apply(self.convert_to_unix).to_numpy()\n",
    "\n",
    "        # Finding time intervals\n",
    "        difference_array = np.append(np.array([]), np.diff(unixCol))\n",
    "        timeFrame = (pd.DataFrame().assign(index = np.arange(1,len(unixCol)), Times_diff = difference_array))\n",
    "        dateSurprise = timeFrame.assign(surprise = self.surprise_estimator(timeFrame.get(\"Times_diff\")))\n",
    "        print(dateSurprise.sort_values(by = ['surprise'], ascending = False))\n",
    "        return dateSurprise.get(\"surprise\")\n",
    "\n",
    "    \n",
    "    # If date-value is given as a string, convert to date- time format first\n",
    "    # TODO: make adaptable to all forms of date/time?\n",
    "    def convert_to_date_time(self,column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Converts a column to date time with format('%Y%m%d %H:%M:%S')\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A 'datetime-valued' Series to be converted to type = datetime\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        column: Pandas Series\n",
    "            A datetime Pandas Series of type datetime \n",
    "        \"\"\"\n",
    "        return pd.to_datetime(column, format='%Y%m%d %H:%M:%S')\n",
    "\n",
    "        \n",
    "    # Converting the date to unix format for ease of calculations    \n",
    "    def convert_to_unix(self,value: datetime) -> datetime:\n",
    "        \"\"\"\n",
    "        Returns a unix timestamp version of a given timestamp.\n",
    "        \n",
    "        Common issues: Passed value is not of type datetime / Passed value is before 1970 will both yield errors.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value: datetime - An ordinary datetime timestamp\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value: datetime\n",
    "            The number of total seconds since January 1st, 1970\n",
    "        \"\"\"\n",
    "        return (value - datetime(1970, 1, 1)).total_seconds()\n",
    "\n",
    "    \n",
    "    # A grouping of the entire kernel estimation process\n",
    "    def surprise_of_table(self, index: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns Surprise values for a whole table based off Kernel Density estimations (per column).\n",
    "        \n",
    "        Common issues: Table should have a set index, or one should be specified for the index argument. Not having\n",
    "        such an index will cause trouble when grouping all column Estimations into a single DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index: String - An optional index to set for the new table containing per column suprise: otherwise current set index will be used (default = None)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        metric: Pandas Series\n",
    "            A Series containing each value index and its corresponding Kernel Density Estimation.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Preprocessing data\n",
    "        temp = self.kernel_prep(self.data)\n",
    "        copyTable = copy.copy(self.data)\n",
    "        \n",
    "        # Timestamp anomaly\n",
    "        if self.timestamp_anom == True:\n",
    "            copyTable[self.timestamp] = self.date_time_interval_anomaly(self.data[self.timestamp])\n",
    "\n",
    "        # Checking if index given, if it isn't will just use Table's current default index\n",
    "        if type(index) == str:\n",
    "            index = self.data.get(index)\n",
    "        else:\n",
    "            index = self.data.index\n",
    "            \n",
    "\n",
    "        #Obtaining surprise of every individual column\n",
    "        sum_surprise  = pd.Series(np.zeros(self.data.shape[0]))\n",
    "        for col in temp.columns:\n",
    "            stat = temp.get(col)\n",
    "            copyTable[col] = self.surprise_estimator(stat)\n",
    "            sum_surprise+=copyTable[col]\n",
    "        \n",
    "        # If we have a TimeSeries column, adding it to sum surprise\n",
    "        if(self.timestamp_anom == True):\n",
    "            sum_surprise += copyTable[self.timestamp]\n",
    "            \n",
    "        # Averaging our surprise so we can sort by it\n",
    "        sum_surprise = sum_surprise.array\n",
    "        \n",
    "        copyTable = copyTable.set_index(index)\n",
    "        copyTable = copyTable.assign(mean_surprise = np.round(sum_surprise/copyTable.shape[1],2))\n",
    "\n",
    "        # Sorting table for easier visualization\n",
    "        copyTable = copyTable.sort_values(by = \"mean_surprise\", ascending  = False)\n",
    "        return copyTable\n",
    "    \n",
    "    \n",
    "    def train_test_anomaly(self):\n",
    "        \"\"\"\n",
    "        Given a preloaded train and test set, calculates metric of surprise based off the difference between Train and Test Kernel densitites\n",
    "        \n",
    "        Common Issues: Since this method is comparing by column, the train and test datasets must have the same columns with the same names\n",
    "        prior to runtime.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surprise: pd.DataFrame\n",
    "                A DataFrame containing the anomaly measure for every single column.\n",
    "        \"\"\"\n",
    "        # Copying Dataset without becoming an alias.\n",
    "        surprise = self.kernel_prep(data = self.testDf)\n",
    "        \n",
    "        # Trimming train and test sets\n",
    "        trainTrimmed = self.kernel_prep(data = self.trainDf)\n",
    "        testTrimmed = self.kernel_prep(data = self.testDf)\n",
    "        \n",
    "        for col in trainTrimmed.columns:\n",
    "            # Finding difference between train and test distributions\n",
    "            model, trainKernel = self.kernel_decider(trainTrimmed[col])\n",
    "            \n",
    "            # The observed densities.\n",
    "            model2, testKernel = self.kernel_decider(testTrimmed[col])\n",
    "            \n",
    "            # The expected values for the test-densities if the test set was identical to the training distribution\n",
    "            testKernelExpected = model.score_samples(testTrimmed[col].to_numpy().reshape(-1,1))\n",
    "            difference = pd.Series(data = testKernel-testKernelExpected, index = testTrimmed.index)\n",
    "            difference = difference.replace({np.nan: 0, np.inf: 100000, -np.inf: -100000})\n",
    "            # Computing surprise\n",
    "            surprise[col] = -np.log2(self.return_p_value(difference))\n",
    "            \n",
    "        return surprise\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_Analysis(Anomaly):\n",
    "    \"\"\"\n",
    "    Row-based outlier techniques that utilizes dimensionality reduction to understand systematic bias by row.\n",
    "    The dimensionality reduction is a great way of finding mutli-dimensional outliers without the destruction of computation speed.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    pcaData = None\n",
    "    threshold = None\n",
    "    trainDf  = None\n",
    "    testDf = None\n",
    "    replace_nan = None\n",
    "    \n",
    "    #Initializing dataset, train, and test sets.\n",
    "    def __init__(self, dataInit: pd.DataFrame = None, train:pd.DataFrame = None, test:pd.DataFrame = None, s:float = .95, replace_nan_with = False):\n",
    "        \"\"\"\n",
    "        Reduces dimensionality while maintaining s% of variablity, then uses dimension mixtures called PC's to find systematic bias across every \n",
    "        column in the row (checks for anomaly by row)\n",
    "        \n",
    "        Common issues: The value of s (the variability) should be between 0 and 1 (it is a proportion) and not setting it in\n",
    "        this range will yield an error. Additionally, the PCA class alone does not have a train and test set evaluation and thus\n",
    "        does not have train and test set variables, so attempting to set them in the constructor will also yield an error.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataInit: Pandas DataFrame - The Data that will be used for the PCa evaluation\n",
    "        s: float - The floating point hyperparameter for the percent of variance (information) in the dimensionality reduction that you'd like to maintain (default = .95)\n",
    "        replace_nan_with: float - A value to use to replace NaN values instead of just dropping columns that include them (default = False)\n",
    "        \"\"\"\n",
    "        self.data = dataInit\n",
    "        self.threshold = s\n",
    "        self.trainDf = train\n",
    "        self.testDf = test\n",
    "        assert self.threshold > 0 and self.threshold < 1, \"Please ensure your threshold is between 0 and 1\"\n",
    "        self.replace_nan = replace_nan_with\n",
    "        \n",
    "\n",
    "    # Drops non-numerical and nan values from a table\n",
    "    def pca_prep(self, data: pd.DataFrame = None)-> pd.DataFrame: \n",
    "        \"\"\"\n",
    "        Removes non-numerical columns and NaN from the dataset for proccessing purposes\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        data: Pandas DataFrame - A Table to prep for PCA\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        tabl: pandas DataFrame\n",
    "            A DataFrame containing the processed data   \n",
    "        \"\"\"\n",
    "        # Finding all numerical components of the table so that pca can function\n",
    "        if type(data) != pd.DataFrame:\n",
    "            data = self.data\n",
    "        \n",
    "        tabl = data.select_dtypes(include = [np.number])\n",
    "        \n",
    "        # Giving user choice to replace missing values instead of dropping them altogether\n",
    "        if type(self.replace_nan) != bool:\n",
    "            self.replace_nan = float(self.replace_nan)\n",
    "            tabl.replace(to_replace = [np.nan], value =[self.replace_nan], inplace=True)\n",
    "        else:\n",
    "            tabl = tabl.dropna(1)\n",
    "        return tabl\n",
    "    \n",
    "    \n",
    "    def obtain_variance_table(self, data: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Evaluates how much variability each Principal Component accounts for, returned in the format of a DataFrame.\n",
    "        \n",
    "        Common issues: You must set a Data Set for this method to run on prior to evaluation. Not doing so will yield an\n",
    "        error.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Pandas DataFrame - Data to obtain the respective variances of (default = self.data)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        infoFrame: Pandas DataFrame\n",
    "            A Table containing each Principal Component and the variance that it accounts for\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensuring that we have our dataset prior to running this method \n",
    "        if type(data) != pd.DataFrame:\n",
    "            data = self.data\n",
    "            \n",
    "        assert type(data) == pd.DataFrame, \"Please set a Data Set prior to running this method\"\n",
    "        \n",
    "        # Scaling and preparing values for PCA\n",
    "        data = self.pca_prep(data)\n",
    "        scaled_data= StandardScaler().fit_transform(data)\n",
    "\n",
    "        # Creating a PCA object \n",
    "        pca = PCA(n_components = (data.shape[1]))\n",
    "        pcaFit = pca.fit_transform(scaled_data)\n",
    "        infoFrame = pd.DataFrame().assign(Column = [\"PC\" + str(i) for i in range(data.shape[1])], Variance_ratio = pca.explained_variance_ratio_ )\n",
    "        return infoFrame\n",
    "    \n",
    "    \n",
    "    def obtain_pca_vals(self, componentNum: int, data = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the Principal Component values for the first n principal components.\n",
    "        \n",
    "        Common issues: the componentNum argument should not exceed the dimensionality (number of columns) of the table.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        componentNum: int - The number of Principal Components you would like to keep\n",
    "        data: Pandas DataFrame - A dataset to find the PCA values for\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        metric: Pandas DataFrame\n",
    "            A Table containing a reduced number of Principal Components\n",
    "        \"\"\"\n",
    "        # Default value\n",
    "        if type(data) == None:\n",
    "            data = self.pcaData\n",
    "            \n",
    "        # Building new model off n Principal Components\n",
    "        pca = PCA(n_components = componentNum)\n",
    "        pcaFit = pca.fit_transform(data)\n",
    "        return pca, pcaFit\n",
    "    \n",
    "    \n",
    "    # Deciding how many columns need to be used: utilizing threashold of 95% of the explained variance\n",
    "    def element_decider(self,infoFrame: pd.DataFrame)-> int:\n",
    "        \"\"\"\n",
    "        Based off the variance table evaluation, decides how many Principal Compoenents need to be kept to maintain variance threshold\n",
    "        \n",
    "        Common issues: Ensure that the Pandas DataFrame you are passing to this method is the variance table obtained \n",
    "        via the obtain_variance_table method. Passing this method a difference table will yield an error.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        infoFrame: Pandas DataFrame - A table containing each Principal Component and the variance it accounts for \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        counter: int\n",
    "            The count of how many sequential Principal compoenents need to be kept to meet the variance threshold\n",
    "        \"\"\"\n",
    "        numSum = 0\n",
    "        counter = 0\n",
    "\n",
    "        # Continuing until we have accounted for 95% of the variance\n",
    "        for i in infoFrame.get(\"Variance_ratio\"):\n",
    "            if(numSum < self.threshold):\n",
    "                numSum += i\n",
    "                counter+=1\n",
    "        return counter\n",
    "\n",
    "    \n",
    "    # Reducing dimensionality of data into pc's, only storing what is neccessary\n",
    "    def reduced_data(self,infoFrame: pd.DataFrame, pcaData, index: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reduces dimensionality of the data to the Class threshold and calculates the values of each of the Principal components\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        infoFrame: Pandas DataFrame - A table with the % of variance accounted for by each Principal Component. Obtained via self.obtain_variance_table()\n",
    "        pcaData: 2 dimensional list - A 2 dimensional list of the scaled Data for processing purprose\n",
    "        index: Pandas Series - A Pandas Series to use as the index for the data\n",
    "    \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pcaFrame: Pandas DataFrame\n",
    "            A Pandas DataFrame containing the values from the reduced Principal Components.\n",
    "        \"\"\"\n",
    "\n",
    "        numCols = self.element_decider(infoFrame)\n",
    "        pca, pcaVals = self.obtain_pca_vals(numCols, pcaData)\n",
    "        pcaFrame = pd.DataFrame(pcaVals)\n",
    "\n",
    "        # Dealing with potential index issues\n",
    "        pcaFrame = pcaFrame.set_index(index)\n",
    "        return pca, pcaFrame\n",
    "    \n",
    "    \n",
    "    #Summing p-values because PCA serves to check for systematic bias, whereas kernel density checks for accuracy\n",
    "    def sum_rows(self,pcaVals: pd.DataFrame) -> np.array:\n",
    "        \"\"\"\n",
    "        Sums the absolute value of Principal Components (due to Orthogonality, direction does not matter)s\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pcaVals: Pandas DataFrame - A table full of values returned from a PCA evaluation (self.reduced_data)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        metric: Pandas Series\n",
    "            A Series containing each value index and its corresponding Kernel Density Estimation.\n",
    "        \"\"\"\n",
    "        sumArray = np.zeros(pcaVals.shape[0])\n",
    "        for i in np.arange(pcaVals.shape[1]):\n",
    "            values = pcaVals.get(str(i)).array\n",
    "            sumArray += abs(values)\n",
    "        sumArray /= pcaVals.shape[1]\n",
    "        #After obtaining sum, the average deviation from the expected value is averaged out, not taking in absolute value\n",
    "        # to check for systematic error\n",
    "        return sumArray\n",
    "\n",
    "\n",
    "    # Tests for systematic bias by row\n",
    "    def pca_row_outliers(self, pcaVals: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes average anomaly (Surprise) on table full of Principal Components, by row.\n",
    "        \n",
    "        Common issues: An extremely low P-value may yield a Surprise of infinity: this still does not guarantee anomaly,\n",
    "        but rather means that the result is exponentially unlikely based off the computation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pcaVals: Pandas DataFrame - A DataFrame full of Principal Components and their corresponding scores.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        newVals: Pandas DataFrame\n",
    "            A DataFrame containing average anomaly estimations by row under the column of surprise.\n",
    "        \"\"\"\n",
    "        P_val_table = pd.DataFrame()\n",
    "\n",
    "        #Creating a table of all the PCA p-values\n",
    "        for col in np.arange(0,pcaVals.shape[1]):\n",
    "            P_vals =  self.return_p_value(pcaVals.get(col))\n",
    "            i = str(col)\n",
    "            P_val_table[i] = P_vals\n",
    "        totalVar = self.sum_rows(P_val_table)\n",
    "\n",
    "        #Calculating surprise by taking negative log\n",
    "        newVals = pcaVals.assign(Surprise = -np.log2(totalVar))\n",
    "        newVals = newVals.sort_values(by = \"Surprise\", ascending = False)\n",
    "        return newVals\n",
    "\n",
    "    \n",
    "    # Master method to run PCA as a whole\n",
    "    def surprise_on_table(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Runs the entire PCA process on the data it has fit itself onto the loaded dataset\n",
    "        \n",
    "        Common issues: Ensure given index is a valid Column within your table. Data Set must also be initialized prior to run.\n",
    "        See other function documentations for more common issues.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index: string - The name of a valid column that you would like to serve as your index.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_pca: Pandas DataFrame\n",
    "            A table Containing anomaly projections based on Principal Component Analysis value, sorted in order by surprise\n",
    "        \"\"\"\n",
    "        processing_table = self.pca_prep(self.data)\n",
    "        variance_table = self.obtain_variance_table(self.data)\n",
    "        pca, pcaVals = self.reduced_data(variance_table, StandardScaler().fit_transform(processing_table), self.data.index)\n",
    "        new_pca = self.pca_row_outliers(pcaVals)\n",
    "        return new_pca\n",
    "    \n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Utilizes PCA to find difference between multi-dimensional shape of one distribution vs. the other.\n",
    "        \n",
    "        Common issues: Ensure given index is a valid Column within your table. Train and Test must be initialized prior to run.x\n",
    "        See other function documentations for more common issues.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index: string - The name of a valid column that you would like to serve as your index.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        new_pca: Pandas DataFrame\n",
    "            A table Containing anomaly projections of the test set based on Principal Component Analysis value, sorted in order by surprise\n",
    "        \"\"\"\n",
    "        # Preproccessing and varifying there are the same columns\n",
    "        processing_table = self.pca_prep(self.trainDf)\n",
    "        processing_test = self.pca_prep(self.testDf)\n",
    "        \n",
    "        # Removing all nonmutual columns\n",
    "        removeCols = []\n",
    "        for a in processing_test.columns:\n",
    "            if a not in processing_table.columns:\n",
    "                removeCols.append(a)\n",
    "        processing_test = processing_test.drop(columns = removeCols)\n",
    "        \n",
    "        removeColumns = []\n",
    "        for i in processing_table.columns:\n",
    "            if i not in processing_test.columns:\n",
    "                removeColumns.append(i)\n",
    "        processing_table = processing_table.drop(columns = removeColumns)\n",
    "        \n",
    "        # Building a Training Model so that we can use it to transform test set values\n",
    "        variance_table = self.obtain_variance_table(processing_table)\n",
    "        pca, pcaVals = self.reduced_data(variance_table, StandardScaler().fit_transform(processing_table), self.trainDf.index)\n",
    "        \n",
    "        # The observed fit onto the test set, which will be compared to the expected fit\n",
    "        v_table = self.obtain_variance_table(processing_test)\n",
    "        model, values = self.reduced_data(v_table, StandardScaler().fit_transform(processing_test), self.testDf.index)\n",
    "\n",
    "        # Transforming test set values, the expected fit.\n",
    "        transformed_test = pd.DataFrame(pca.transform(processing_test)).set_index(self.testDf.index)\n",
    "        \n",
    "        # Finding difference between computations and obtaining anomaly\n",
    "        test_diff = transformed_test.subtract(values)\n",
    "        test_diff = test_diff.replace({np.nan:0})\n",
    "        new_pca = self.pca_row_outliers(test_diff)\n",
    "        return new_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet to be documented\n",
    "class Categorical(Anomaly):\n",
    "    \"\"\"Uses dynamically built data \"grammar conventions\" to find outliers based on defiance of strict structures\"\"\"\n",
    "    \n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None \n",
    "    userParams = None\n",
    "    userFactor = None\n",
    "    \n",
    "    # Initializing data, train and test sets, and potential outlier characters/words\n",
    "    def __init__(self,data_init = None , train_init = None , test_init = None, user_params_init:[] = None, user_surprise_factor:int = None):\n",
    "        \"\"\"\n",
    "         Object constructor which allows user to set dataset and initialize train+test sets. \n",
    "        Keep in mind the dataset can also be loaded by means of any method in the base Anomaly class.\n",
    "        You also don't have to initialize your train and test sets right away and can use our random splitter to do so prior to running the Categorical Anomaly itself!\n",
    "        User also given the option to initialize values that if found add to the anomaly factor by n automatically. This can be done with user_params_init\n",
    "        in the constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_init: Pandas DataFrame - The dataset you would like to fit Kernel Density to (default = None)\n",
    "        train_set: Pandas DataFrame  - The train set you would like to use for Kernel Density purposes (default = None)\n",
    "        test_set: Pandas DataFrame  - The test set you would like to use for Kernel Density purposes (default = None)\n",
    "        user_params_init: list - A list of numerical or string values that, if found in the dataset, automatically increase surprise by user_surprise_factor points\n",
    "        user_surprise_factor: int - The factor to increase a value by if user_params_init has been found in a given data cell\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data_init\n",
    "        self.trainDf = train_init\n",
    "        self.testDf = test_init\n",
    "        userParams = user_params_init\n",
    "        userFactor = user_surprise_factor\n",
    "        \n",
    "     \n",
    "    # Option to initialize user params during runtime\n",
    "    def initialize_user_params(self,params: [], factor: np.number):\n",
    "        \"\"\"\n",
    "        Used to initialize parameters that automatically lead to increase in Anomaly at any point in the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params: list - A list of numerical or string values that, if found in the dataset, automatically increase surprise by n = factor points\n",
    "        factor: int - The factor to increase a value by if a member of params has been found in a data cell.\n",
    "        \"\"\"\n",
    "        self.userParams = params\n",
    "        self.userFactor = factor\n",
    "        \n",
    "    \n",
    "    # Will examine whether or not a column is categorical, giving the user the opportunity to add additional numeric columns\n",
    "    # Should it automatically identify all String columns as categorical?\n",
    "    def identify_categorical(self,surpriseFrame: pd.DataFrame, supress: bool = False)-> []:\n",
    "        \"\"\"\n",
    "        Identifies the Categorical columns in a table. Default = all string columns, with potential of user input to add other types.\n",
    "        \n",
    "        Common issues: Will automatically identify all columns of string type as categorical.  \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        SurpriseFrame: pd.DataFrame: A table to identify the categorical columns of\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        categorical_list: list\n",
    "            A list of all the Categorical columns in the dataset\n",
    "        \"\"\"\n",
    "        categorical_list = []\n",
    "        for col in surpriseFrame.columns:\n",
    "            if(not(is_numeric_dtype(surpriseFrame[col]))):\n",
    "                categorical_list.append(col)\n",
    "\n",
    "        # Allows fixing of default assumption that numeric columns aren't categorical\n",
    "        print(\"Are there any numeric Columns you would consider categorical?(yes/no)\")\n",
    "        while input().upper() == \"YES\" and (supress == False):\n",
    "            print(\"Enter the name of one such column:\")\n",
    "            categorical_list.append(input())\n",
    "            print(\"Any more?(yes/no)\")\n",
    "        return categorical_list\n",
    "    \n",
    "    \n",
    "    # Returns suprise of type classification\n",
    "    def types(self,column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Tests the data type 'grammatical rule', calculating anomaly based off how often certain data types are followed\n",
    "        \n",
    "        Common issues: Only looks at 3 categories: boolean, numeric, or string.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test for types \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surpriseVal: Pandas Series\n",
    "            A measure of anomaly for the types test of the column using the metric of surprise, ordered by index\n",
    "        \"\"\"\n",
    "        value_types = column.apply(self.classifier)\n",
    "        counts  = value_types.value_counts(normalize = True)\n",
    "        index = counts.index\n",
    "        values = counts.values\n",
    "        probs = value_types.apply(self.give_prob, args = (np.array(index), np.array(values)))\n",
    "        surpriseVal = probs.apply(self.surprise)\n",
    "        return surpriseVal\n",
    "\n",
    "    \n",
    "    # Obtains the type of value, even if it is currently contained within a string\n",
    "    def classifier(self, value) -> str:\n",
    "        \"\"\"\n",
    "        Identifies the data type of a certain value (even if stored in a string)\n",
    "        \n",
    "        Common issues: Only identifies 3 data types: number, string, boolean.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value: A primitive python value\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret: String\n",
    "            A String represetning the data type of the input value\n",
    "        \"\"\"\n",
    "        value = str(value)\n",
    "        # Boolean check done manually: this is an easy check\n",
    "        if(('True' in value) or ('False' in value )):\n",
    "            return 'boolean'\n",
    "        else:\n",
    "            if(value.isnumeric()):\n",
    "                return 'number'\n",
    "            else:\n",
    "                return 'string'\n",
    "\n",
    "            \n",
    "    # Takes in a column and returns the surprise of each nan value being present (True) or not being present (False)\n",
    "    def nans(self, column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Tests data for missing (NaN) values, quantifying anomaly based off the regularity of missing values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test for nan values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surpriseVal: Pandas Series\n",
    "            A Series representing anomaly estimations for missing and nonmissing values based off the regularity of nans\n",
    "        \"\"\"\n",
    "        nan_values = column.apply(self.is_nan)\n",
    "        counts  = nan_values.value_counts(normalize = True)\n",
    "        index = counts.index\n",
    "        values = counts.values\n",
    "        probs = nan_values.apply(self.give_prob, args = (np.array(index), np.array(values)))\n",
    "        surpriseVal = probs.apply(self.surprise)\n",
    "        return surpriseVal\n",
    "\n",
    "    \n",
    "    # Takes in a column and returns the surprise of the length of each value in the column: the first and simplest of probabilistic tests\n",
    "    def len_count(self, column: pd.Series)-> pd.Series:\n",
    "        \"\"\"\n",
    "        Tests for anomaly by sequence length (A standard sequence length being violated could be anomalous)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test for length\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surpriseVal: Pandas Series\n",
    "            A Series representing anomaly estimations for respective values based off standards for sequence length\n",
    "        \"\"\"\n",
    "        column = column.apply(str)\n",
    "        counts = column.apply(len).value_counts(normalize = True)\n",
    "        index = counts.index\n",
    "        values = counts.values\n",
    "        column = column.apply(len)\n",
    "        probs = column.apply(self.give_prob, args = (np.array(index), np.array(values)))\n",
    "        surpriseVal = probs.apply(self.surprise)\n",
    "        return surpriseVal\n",
    "\n",
    "    \n",
    "    # Calculates the surprise of a given value\n",
    "    def surprise(self, value):\n",
    "        \"\"\"\n",
    "        Returns surprise metric\n",
    "        \n",
    "        Common Issues: -log2(0) does not exist, so will return infinite values for p-values that are negligibly close to 0\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value: float - A floating point p-value\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surprise: float\n",
    "            A number representing the entropy/anomaly of the data member\n",
    "        \"\"\"\n",
    "        return -np.log2(value)\n",
    "\n",
    "    \n",
    "    # Given a numerical value, finds it equivalent within the set of indices and assigns it the proper probability\n",
    "    def give_prob(self, value, index, values):\n",
    "        \"\"\"\n",
    "        Maps values to respective probabilities.\n",
    "        \n",
    "        Common issues: value must correspond to a value in the index\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value: primitive - A value corresponding to some element in the index\n",
    "        index: [] - Respective value labels\n",
    "        values:[] - A list of corresponding probabilities to the index list\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret: float\n",
    "            The respective probability of the value inputted\n",
    "        \"\"\"\n",
    "        for num in np.arange(len(index)):\n",
    "            if(value == index[num]):\n",
    "                return values[num]\n",
    "        return values[0]\n",
    "\n",
    "\n",
    "    # NaN's aren't equal to themselves\n",
    "    def is_nan(self, x) -> bool:\n",
    "        \"\"\"\n",
    "        Tests if a value is a Null/NaN value by checking if it is equal to itself\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: primitive value - A value to test for being a null/NaN value\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret: bool\n",
    "            A Boolean representing whether the value is a null/NaN value\n",
    "        \"\"\"\n",
    "        return x!=x\n",
    " \n",
    "\n",
    "    # Checks for special characters within a string, calculating surprise so as to identify which character combinations are chaotic\n",
    "    def special_char(self, column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Tests for anomaly by special character sequence and presence\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test for special characters\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surpriseVal: Pandas Series\n",
    "            A Series representing anomaly estimations for respective values based off standards for special character sequence/ presence\n",
    "        \"\"\"\n",
    "        characters = column.apply(str).apply(self.char_identifier)\n",
    "        counts  = characters.value_counts(normalize = True, dropna = False)\n",
    "        index = counts.index\n",
    "        values = counts.values\n",
    "        probs = characters.apply(self.give_prob, args = (np.array(index), np.array(values)))\n",
    "        surpriseVal = probs.apply(self.surprise)\n",
    "        return surpriseVal\n",
    "\n",
    "    \n",
    "    # Checks if a single entry of any data type contains special symbols and returns all that it contains\n",
    "    def char_identifier(self, entry) -> str:\n",
    "        \"\"\"\n",
    "        Finds the sequence of special characters in a given entry\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        entry: String - A String to test for special characters\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret_string: String\n",
    "            The sequence of special characters in the entry.\n",
    "        \"\"\"\n",
    "        charList = np.array(['<', '>', '!', '#','_','@','$','&','*','^', ' ', '/', '-','\"','(', ',', ')', '?', '.'])\n",
    "        ret_string = \"\"\n",
    "        for i in charList:\n",
    "            if(i in entry):\n",
    "                ret_string += i\n",
    "        return ret_string\n",
    "\n",
    "    \n",
    "    # Simpler approach here: if the value counts of certain elements are greater when they should be unique, they are more suprising\n",
    "    # If they are non-unique when they are supposed to be unique, also more surprising. Done with binary classification\n",
    "    def uniques(self, column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Tests for anomaly by uniqueness of value (A value being unique in a series full of nonunique could be anomalous)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test for uniqueness\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surpriseVal: Pandas Series\n",
    "            A Series representing anomaly estimations for respective values based off standards for uniqueness\n",
    "        \"\"\"\n",
    "        # Counting number of each value and returning whether or not it is a singular unique value,\n",
    "        #then counting truly unique values\n",
    "        vals = column.value_counts(dropna = False).apply(self.isunique)\n",
    "        vals = column.apply(self.unique_assignment, args = [vals])\n",
    "        counts = vals.value_counts(normalize = True, dropna = False)\n",
    "        index = counts.index\n",
    "        values = counts.values\n",
    "        probs = vals.apply(self.give_prob, args = (np.array(index), np.array(values)))\n",
    "        surpriseVal = probs.apply(self.surprise)\n",
    "        # Note: if all values unique/non unique this will provide definite outcome because no room for uncertainty\n",
    "        return surpriseVal\n",
    "\n",
    "    \n",
    "    # Returns whether the count of a value is 1\n",
    "    def isunique(self, val) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if a value is unique\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        val: A value count - A value count to be tested\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ret: Boolean\n",
    "            A True/False value corresponding to uniqueness of the value whose count was given\n",
    "            \"\"\"\n",
    "        return (val == 1)\n",
    "\n",
    "    \n",
    "    # Maintains individual values without grouping while assigning unique / nonunique probabilities. To be used on original column\n",
    "    def unique_assignment(self, val, column):\n",
    "        \"\"\"\n",
    "        Maps a unique/nonunique label to corresponding probability\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        val: boolean - A value indicating uniqueness\n",
    "        column: Series - A column containing probabilities for unique(True) and non-unique(False) elements.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value: float\n",
    "            Corresponding probability of unique/ nonunique value\n",
    "        \"\"\"\n",
    "        if self.is_nan(val):\n",
    "            value = column[pd.Series(column.index).apply(self.is_nan).tolist()]\n",
    "        else:\n",
    "            value = column[column.index == val]\n",
    "        return value.iloc[0]\n",
    "  \n",
    "\n",
    "    # Obtains a date time object and treats this as numerical rather than categorical value\n",
    "    def surprise_on_table(self, supress = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Builds grammatical expectations for table, calculating Surprise based off the commonality with which such tendencies are broken\n",
    "        \n",
    "        Common Issues: This is a Categorical method and does not take into account numerical magnitude or datetime interval.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        table: Pandas DataFrame - A table to test for anomaly\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        table: Pandas DataFrame\n",
    "            The modified table with the each value being replaced by its respective measure of anomaly\n",
    "        \"\"\"\n",
    "        table = self.data\n",
    "        cols = self.identify_categorical(table, supress)\n",
    "        for col in cols:\n",
    "            # Obtaining individual relative entropies, averaging them out, finding their p-values and calculating final surprise\n",
    "            values = table.get(col)\n",
    "            factors = values.apply(self.apply_user_factor)\n",
    "            temp = (self.uniques(values)+ self.special_char(values)+ self.nans(values) + self.types(values)+ self.len_count(values))/5\n",
    "            \n",
    "            # Finding Surprise and adding user defined factors\n",
    "            table[col] = -np.log2(self.return_p_value(temp)) + factors\n",
    "        \n",
    "        # Replacing any uncomputable values with 0\n",
    "        table = table.replace({np.nan:0})    \n",
    "        return table\n",
    "\n",
    "    \n",
    "    def apply_user_factor(self, value) -> int:\n",
    "        \"\"\"\n",
    "        Used to initialize parameters that automatically lead to increase in Anomaly at any point in the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value: Python Primitive value - A value to search userParams for, applying the userFactor if found\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Factor: int\n",
    "            0 if the userFactor has not been applied, and self.userFactor if it has (based off whether value in userParams)\n",
    "        \"\"\"\n",
    "        # Ensuring a user factor exists\n",
    "        if self.userParams != None and value in self.userParams:\n",
    "            return self.userFactor\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    # Proxy train test anomaly: checks test set anomaly as a function of their combined distribution\n",
    "    def train_test_anomaly(self):\n",
    "        \"\"\"\n",
    "        A proxy train test anomaly function that evaluates test set categorical anomaly as a subset of the combined train and test\n",
    "        set rules. For more information, see Categorical.surprise_on_table() method.\n",
    "        \n",
    "        Common issues: Train and Test Sets must be initialized prior to run of this method and must contain some categorical data\n",
    "        used to evaluate. For this purpose, any non-numerical data is by default rendered Categorical Data. Other columns could be\n",
    "        rendered Categorical by the user.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        surprise: Pandas DataFrame\n",
    "            Evaluated Anomaly metrics for test set as a function of surprise (entropy).\n",
    "        \"\"\"\n",
    "        # Ensuring train and test sets have been initialized\n",
    "        assert type(self.trainDf) == pd.DataFrame and type(self.testDf) == pd.DataFrame, \"Train and test sets must be initialized prior to run\"\n",
    "        \n",
    "        # Grouping train and test sets to form a distribution\n",
    "        total_set  = pd.concat([self.trainDf, self.testDf], ignore_index = False)\n",
    "        \n",
    "        # Calculating entropy of joint distribution\n",
    "        surprise = self.surprise_on_table(total_set)\n",
    "        \n",
    "        # Returning only train-set Surprise metric values\n",
    "        return surprise.iloc[self.testDf.index.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Categorical()\n",
    "cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.data = pd.read_excel(\"sampleDataSet.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                    Email  Profession       Hometown  Height       Salary  \\\n",
       " 148         Rob@gmail.com          DJ  San Diego, CA    64.0      45000.0   \n",
       " 70          Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 118         Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 137     Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 85          Rob@gmail.com          DJ  San Diego, CA    73.0      60000.0   \n",
       " 40          Gil@gmail.com      Doctor  San Diego, CA    52.0      60000.0   \n",
       " 94         Ross@gmail.com  Programmer  San Diego, CA    72.0      55000.0   \n",
       " 13         Ross@gmail.com      Doctor  San Diego, CA    52.0      45000.0   \n",
       " 79         Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " 77       Robert@gmail.com       Nurse  San Diego, CA    75.0      50000.0   \n",
       " 116     Herbert@gmail.com      Doctor  San Diego, CA    74.0      45000.0   \n",
       " 66      Herbert@gmail.com      Doctor  San Diego, CA    74.0      45000.0   \n",
       " 125     Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 106     Herbert@gmail.com      Doctor  San Diego, CA    74.0      45000.0   \n",
       " 109        Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " 136         Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 130         Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 87       Robert@gmail.com       Nurse  San Diego, CA    75.0      50000.0   \n",
       " 75          Rob@gmail.com          DJ  San Diego, CA    73.0      60000.0   \n",
       " 52       Robert@gmail.com      Doctor  San Diego, CA    70.0      45000.0   \n",
       " 131        Ross@gmail.com  Programmer  San Diego, CA    77.0      60000.0   \n",
       " 157     Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 34          Gil@gmail.com      Doctor  San Diego, CA    69.0      50000.0   \n",
       " 155        Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " 96      Herbert@gmail.com      Doctor  San Diego, CA    74.0      45000.0   \n",
       " 102      Robert@gmail.com      Doctor  San Diego, CA    70.0      45000.0   \n",
       " 9           Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 55          Rob@gmail.com          DJ  San Diego, CA    73.0      60000.0   \n",
       " 7           Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 151        Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " ..                    ...         ...            ...     ...          ...   \n",
       " 113         Gil@gmail.com       Nurse  San Diego, CA    71.0      50000.0   \n",
       " 95          Rob@gmail.com          DJ  San Diego, CA    73.0      60000.0   \n",
       " 88          Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 83          Gil@gmail.com       Nurse  San Diego, CA    71.0      50000.0   \n",
       " 21       Robert@gmail.com      Doctor  San Diego, CA    61.8      45000.0   \n",
       " 51         Ross@gmail.com          DJ  San Diego, CA    71.0  850850043.0   \n",
       " 12          Gil@gmail.com          DJ  San Diego, CA    55.0      60000.0   \n",
       " 117      Robert@gmail.com       Nurse  San Diego, CA    75.0      50000.0   \n",
       " 81      Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 139        Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " 90          Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 147        Ross@gmail.com          DJ  San Diego, CA    77.0      60000.0   \n",
       " 156         Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 152         Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 150         Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 74         Ross@gmail.com  Programmer  San Diego, CA    72.0      55000.0   \n",
       " 36          Rob@gmail.com  Programmer  San Diego, CA    71.0      60000.0   \n",
       " 154  Gil@syntheticData*!@  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 11       Robert@gmail.com  Programmer  San Diego, CA    58.0      55000.0   \n",
       " 42          Rob@gmail.com  Programmer  San Diego, CA    62.4      50000.0   \n",
       " 71      Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 39      Herbert@gmail.com      Doctor  San Diego, CA    74.0      55000.0   \n",
       " 128         Rob@gmail.com      Doctor  San Diego, CA    64.0      45000.0   \n",
       " 49      Herbert@gmail.com       Nurse  San Diego, CA    72.0       4500.0   \n",
       " 16       Robert@gmail.com          DJ  San Diego, CA    62.7      60000.0   \n",
       " 78          Gil@gmail.com  Programmer  San Diego, CA    76.0      55000.0   \n",
       " 121     Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 61      Herbert@gmail.com       Nurse   San Digo. CA    61.0      50000.0   \n",
       " 72       Robert@gmail.com      Doctor  San Diego, CA    70.0      45000.0   \n",
       " 33       Robert@gmail.com           *  San Diego, CA    68.0      45000.0   \n",
       " \n",
       "      Weight  \n",
       " 148     150  \n",
       " 70      150  \n",
       " 118     181  \n",
       " 137     180  \n",
       " 85      223  \n",
       " 40      223  \n",
       " 94      181  \n",
       " 13      150  \n",
       " 79      223  \n",
       " 77      180  \n",
       " 116     150  \n",
       " 66      150  \n",
       " 125     180  \n",
       " 106     150  \n",
       " 109     223  \n",
       " 136     150  \n",
       " 130     181  \n",
       " 87      180  \n",
       " 75      223  \n",
       " 52      150  \n",
       " 131     223  \n",
       " 157     180  \n",
       " 34      180  \n",
       " 155     223  \n",
       " 96      150  \n",
       " 102     150  \n",
       " 9       150  \n",
       " 55      223  \n",
       " 7       181  \n",
       " 151     223  \n",
       " ..      ...  \n",
       " 113     180  \n",
       " 95      223  \n",
       " 88      181  \n",
       " 83      180  \n",
       " 21      150  \n",
       " 51      181  \n",
       " 12      223  \n",
       " 117     180  \n",
       " 81      180  \n",
       " 139     223  \n",
       " 90      150  \n",
       " 147     223  \n",
       " 156     150  \n",
       " 152     150  \n",
       " 150     181  \n",
       " 74      181  \n",
       " 36      223  \n",
       " 154     181  \n",
       " 11      181  \n",
       " 42      180  \n",
       " 71      180  \n",
       " 39      181  \n",
       " 128     150  \n",
       " 49      150  \n",
       " 16      223  \n",
       " 78      181  \n",
       " 121     180  \n",
       " 61      180  \n",
       " 72      150  \n",
       " 33    15000  \n",
       " \n",
       " [126 rows x 6 columns],\n",
       "                  Email      Profession       Hometown  Height   Salary  Weight\n",
       " 22       Gil@gmail.com           Nurse  San Diego, CA    72.0  50000.0     180\n",
       " 141  Herbert@gmail.com           Nurse   San Digo. CA    61.0  50000.0     180\n",
       " 65       Rob@gmail.com              DJ  San Diego, CA    73.0  60000.0     223\n",
       " 43   Herbert@gmail.com              DJ  San Diego, CA    62.7  55000.0     181\n",
       " 0                  NaN             NaN            NaN    80.0      NaN     170\n",
       " 63       Gil@gmail.com           Nurse  San Diego, CA    71.0  50000.0     180\n",
       " 31   Herbert@gmail.com      Programmer  San Diego, CA    66.0  55000.0     181\n",
       " 38                   5  kfhjkfhjfhsjkh  San Diego, CA    73.0  50000.0     180\n",
       " 92    Robert@gmail.com          Doctor  San Diego, CA    70.0  45000.0     150\n",
       " 41      Ross@gmail.com           Nurse  San Diego, CA    63.0  45000.0     150\n",
       " 134      Gil@gmail.com      Programmer  San Diego, CA    76.0  55000.0     181\n",
       " 123     Ross@gmail.com              DJ  San Diego, CA    77.0  60000.0     223\n",
       " 26   Herbert@gmail.com           Nurse  San Diego, CA    74.5  50000.0     180\n",
       " 37   Herbert@gmail.com              DJ  San Diego, CA    72.0  45000.0     150\n",
       " 140      Rob@gmail.com          Doctor  San Diego, CA    64.0  45000.0     150\n",
       " 108      Gil@gmail.com      Programmer  San Diego, CA    76.0  55000.0     181\n",
       " 119     Ross@gmail.com              DJ  San Diego, CA    77.0  60000.0     223\n",
       " 29      Ross@gmail.com          Doctor  San Diego, CA    64.0  45000.0     150\n",
       " 19       Rob@gmail.com      Programmer  San Diego, CA    62.1  55000.0     181\n",
       " 30       Rob@gmail.com           Nurse  San Diego, CA    65.0  50000.0     180\n",
       " 60       Rob@gmail.com          Doctor  San Diego, CA    64.0  45000.0     150\n",
       " 101  Herbert@gmail.com           Nurse   San Digo. CA    61.0  50000.0     180\n",
       " 114     Ross@gmail.com      Programmer  San Diego, CA    72.0  55000.0     181\n",
       " 103      Gil@gmail.com           Nurse  San Diego, CA    71.0  50000.0     180\n",
       " 44   Herbert@gmail.com          Doctor  San Diego, CA    62.4  60000.0     223\n",
       " 46       Gil@gmail.com      Programmer  San Diego, CA    62.1  50000.0     180\n",
       " 25   Herbert@gmail.com          Doctor  San Diego, CA    75.0  45000.0     150\n",
       " 80       Rob@gmail.com          Doctor  San Diego, CA    64.0  45000.0     150\n",
       " 56   Herbert@gmail.com          Doctor  San Diego, CA    74.0  45000.0     150\n",
       " 107   Robert@gmail.com           Nurse  San Diego, CA    75.0  50000.0     180\n",
       " 126      Gil@gmail.com      Programmer  San Diego, CA    76.0  55000.0     181\n",
       " 138      Gil@gmail.com      Programmer  San Diego, CA    76.0  55000.0     181)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.assign_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet to be documented\n",
    "class MultiDimCategorical(Categorical):\n",
    "    \"\"\"\n",
    "    Utilizes the idea of mutual entropy to build first order and 2nd order approximations for a \n",
    "    given column based on randomly chosen/handpicked context. Only runs a Train/Test Evaluation: not in Initial Report.\n",
    "    \"\"\"\n",
    "    data = None\n",
    "    train = None\n",
    "    test = None \n",
    "    \n",
    "    # Initializing data, train and test sets, and potential outlier characters/words\n",
    "    def __init__(self,data_init  = None , train_init = None , test_init = None, ):\n",
    "        \"\"\"\n",
    "         Object constructor which allows user to set dataset and initialize train+test sets. \n",
    "        Keep in mind the dataset can also be loaded by means of any method in the base Anomaly class.\n",
    "        You also don't have to initialize your train and test sets right away and can use our random splitter to do so prior to running the Categorical Anomaly itself!\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_init: Pandas DataFrame - The dataset you would like to fit Kernel Density to\n",
    "        train_set: Pandas DataFrame  - The train set you would like to use for Kernel Density purposes (default = None)\n",
    "        test_set: Pandas DataFrame  - The test set you would like to use for Kernel Density purposes (default = None)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data_init\n",
    "        self.train = train_init\n",
    "        self.test = test_init\n",
    "\n",
    "        \n",
    "    # Finds the most chaotic combinations (meaning we actually have distribution on our hands) and returns them as array\n",
    "    # To be used for naive Bayes model\n",
    "    def obtain_random_functional_cols(frame: pd.DataFrame, test_column: pd.Series) -> (str, str): \n",
    "        \"\"\"\n",
    "        Returns 2 potential columns that can be used for building a counts distribution. In the Process, eliminates all columns\n",
    "        that are completely unique, equal to the current test column, or are completely non-unique so that a counts distribution can be built.\n",
    "        \n",
    "        Common Issues: Does not eliminate issue of heavy correlation between the columns as some columns may be categorical,\n",
    "        so the later application of naive bayes might be too naive (new information is weighted too strongly)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: Pandas DataFrame - A table from which to obtain the columns\n",
    "        test_column: Pandas Series - The test column which is used for the mutual counts distribution, passed so that it will not be selected.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cols[point], cols[point2]: 2 element tuple of strings\n",
    "                2 fitting column names to be used to build our counts distribution\n",
    "        \"\"\"\n",
    "        cols = []\n",
    "        for x in frame.columns:  \n",
    "            # Number of unique values, typecasted as an integer\n",
    "            val = len(frame.get(x).value_counts())\n",
    "            # Taking out edge cases\n",
    "            if(val != 1 and val < len(x) and x != test_column):\n",
    "                cols.append(x)\n",
    "        point = np.random.randint(0, len(cols))\n",
    "        point2 = np.random.randint(0, len(cols))\n",
    "        while(point == point2):\n",
    "            point2 = np.random.randint(0, len(cols))   \n",
    "        return cols[point], cols[point2]\n",
    "\n",
    "    \n",
    "    # Categorical only DataFrame to be used later for proccessing purposes\n",
    "    def returnCategoricalFrame(self,frame: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns only the Categorical Columns in a Pandas DataFrame\n",
    "        \n",
    "        Common Issues: Mixed-type columns with String inputs are automatically identified as Categorical, and no Numeric \n",
    "        Inputs found to be Categorical unless specified so during runtime.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: Pandas DataFrame - A table to search for categorical column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cols: Pandas DataFrame\n",
    "            A table consisting of categorical columns only\n",
    "        \"\"\"\n",
    "        cols = identifyCategorical(frame)\n",
    "        return (frame[cols]).reset_index()\n",
    "\n",
    "    \n",
    "    # Changing numerical columns to behave as categorical columns for purpose of predicting categorical columns\n",
    "    def modify_numerics(self, frame: pd.DataFrame, categorical_columns: [], test_column: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms numerical values into categorical numbins for counts purposes (numerical values often completely unique)\n",
    "        \n",
    "        Common Issues: Causes a number to lose several potentially useful numerical properties, but needed for counts purpose.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: Pandas DataFrame - A table whose numerical columns will be modified\n",
    "        categorical columns: list - A list of Categorical Columns to ignore throughout numerical proccessing.\n",
    "        test_column: Pandas Series - The tested Series, which will also be ignored during numerical proccessing.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        frame: Pandas DataFrame\n",
    "            A DataFrame whose numerical values have been modified and placed in numbins (numerical -> categorical)\n",
    "        \"\"\"\n",
    "        numericColumns = frame.select_dtypes(include = [np.number])\n",
    "        numericColumns = numericColumns[numericColumns != test_column]\n",
    "        for col in numericColumns:\n",
    "            if(col not in categorical_columns):\n",
    "                frame[col] = frame[col].apply(convert_to_percentile, args = ([frame[col].to_numpy()]))\n",
    "                frame[col] = frame[col].apply(place_in_numbin)\n",
    "        return frame\n",
    "\n",
    "    \n",
    "    # Converts individual values to percentiles\n",
    "    def convert_to_percentile(self, val, values) -> float:\n",
    "        \"\"\"\n",
    "        Finds the Percentile a Value belongs in \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        val: number - A value to find the percentile of \n",
    "        values: list of numbers - A list to use to check the percentile of.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        percentile: float\n",
    "            The Percentile of val as a member of the values list\n",
    "        \"\"\"\n",
    "        return percentileofscore(values, val)\n",
    "\n",
    "\n",
    "    # Places numeric values in separate individual quartiles\n",
    "    def place_in_numbin(self, sval: float) -> int:\n",
    "        \"\"\"\n",
    "        Places a value in a numbin (percentile bins increasing by increments of 10)\n",
    "        \n",
    "        Common Issues: Causes a number to lose several potentially useful numerical properties, but needed for counts purpose.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sval: float - A value to place in a numbin\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numbin: int\n",
    "            A categorical numbin in which the value is placed\n",
    "        \"\"\"\n",
    "        # Returning the quartile in which the number belongs\n",
    "        if(isNan(val)):\n",
    "            return 0\n",
    "        else:\n",
    "            return int(val/10)\n",
    "\n",
    "\n",
    "    # A simple counts-based probability distribution\n",
    "    def zero_order_approx(self, table: pd.DataFrame, column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Builds a probability distribution for the counts of the a given column, to be used in hte first order approximation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        table: Pandas DataFrame - The Table from which to take the desired column for the counts distribution\n",
    "        column: Pandas Series - The Series that is used to build the zero order counts distribution\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        otherCol: Pandas Series\n",
    "                A Series representative of the count distribution (by proportion) of the data\n",
    "        \"\"\"\n",
    "        table = table.replace({np.nan:0})\n",
    "        totalRows = table.shape[0]\n",
    "        counts = table.groupby(column).count()\n",
    "        otherCol = table.columns[0]\n",
    "        return counts[otherCol] / totalRows\n",
    "\n",
    "    \n",
    "    # Yields a Bayesian distribution using the context provided by another column, builds upon zeroOrder in complexity\n",
    "    def first_order_approx(self, data: pd.DataFrame, column: pd.Series, other: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Builds a first order approximation based off the mutual counts distribution built by grouping by both columns\n",
    "        \n",
    "        Common Issues: This may be quite expensive, and if data extremely large could potentially take quite a long time.\n",
    "        In very extreme cases, could cause Kernel to crash.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Pandas DataFrame - The table from which the columns come from\n",
    "        column: Pandas Series - The test column whose mutual distribution we are attempting to discern (in Bayesian thinking, our distribution A in P(A|B))\n",
    "        other: Pandas Series - The column used to give 'context' for the test column (in Bayesian thinking, our distribution B in P(A|B))\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        finalProb: Pandas DataFrame\n",
    "            A table consisting of the mutual probability distribution for a first order approximation.\n",
    "        \"\"\"\n",
    "        # Ensuring that NaN values do not mess up value counts\n",
    "        table= data.replace({np.nan:0})\n",
    "\n",
    "        totalRows = table.shape[0]\n",
    "        # Finding unique combinations to calculate P(B|A)\n",
    "        counts = table.groupby([column,other]).count()\n",
    "        otherCol = table.columns[0]\n",
    "\n",
    "        # Calculating initial probabilities for A and B \n",
    "        initialA = zero_order_approx(table,column)\n",
    "        initialB = zero_order_approx(table,other)\n",
    "\n",
    "        #P(B|A)\n",
    "        conditionalBGivenA = (counts[otherCol] / totalRows)\n",
    "        updatedProb = conditionalBGivenA.multiply(initialA).reset_index().set_index(other)\n",
    "        # Utilizing probability and given knowledge to yield an expected probability\n",
    "        finalProb = pd.DataFrame().assign(data_value = updatedProb.get(column), given_knowledge = updatedProb.index,probability = updatedProb[otherCol].divide(initialB).values)\n",
    "        return finalProb.set_index(\"data_value\")\n",
    "\n",
    "\n",
    "    # Builds a model to obtain context and create a first order surprise approximation for a given column\n",
    "    def surprise_approx(self,column: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Given a Train and Test Set and a column to test, finds metric of anomaly (surprise) between train and test set distributions given first order context\n",
    "        \n",
    "        Common Issues: Train and Test Set must have exact same column names for this purpose, otherwise potential of break.\n",
    "        Functions on one column at a time\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column: Pandas Series - A column to test via first order approx\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        test: Pandas DataFrame\n",
    "            A table modeling difference between train and test sets and approximating metric of surprise.\n",
    "        \"\"\"\n",
    "        # Placing numerical values in bins\n",
    "        self.train = modify_numerics(self.train, returnCategoricalFrame(self.train).columns,column)\n",
    "        self.test = modify_numerics(self.test, returnCategoricalFrame(self.test).columns,column)\n",
    "\n",
    "        # Finding columns that are potential candidates for knowledge base based off the fact that not everything is a unique value\n",
    "        trainCandidate, testCandidate2 = obtain_random_functional_cols(self.train, column)\n",
    "        train_probs = first_order_approx(self.train, column, trainCandidate).reset_index()\n",
    "\n",
    "        # Building a new index which can be used to measure difference between train and test set\n",
    "        train_probs = train_probs.assign(indx = train_probs.get(\"data_value\").apply(str) + \" \" + train_probs.get(\"given_knowledge\").apply(str))\n",
    "        train_probs = train_probs.set_index(\"indx\")\n",
    "\n",
    "        #Building a new index which can be used to measure difference between train and test set\n",
    "        test_probs = first_order_approx(self.test, column, trainCandidate).reset_index()\n",
    "        test_probs = test_probs.assign(indx = test_probs.get(\"data_value\").apply(str) + \" \" + test_probs.get(\"given_knowledge\").apply(str))\n",
    "        test_probs = test_probs.set_index(\"indx\")\n",
    "\n",
    "        # Finding the difference and calculating surprise\n",
    "        test = test_probs.assign(diff = test_probs.get(\"probability\").subtract(train_probs.get(\"probability\"), level = \"indx\", fill_value = 0))\n",
    "        test = test.assign(surprise = surprise(retPVal(test.get(\"diff\"))))\n",
    "        return test.sort_values(by = \"surprise\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report(Anomaly):\n",
    "    \"\"\"An extention of the RunInitial class that offers a more verbose and visual report for an initial anomaly scan\"\"\"\n",
    "    data = None\n",
    "    colDist = None\n",
    "    knalPca = None\n",
    "    categorical = None\n",
    "    report  = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataInit: pd.DataFrame = None, autoInit: bool = True):\n",
    "        \"\"\"\n",
    "        Object constructor which allows user to set dataset and decide whether grouping columns into Numerical, \n",
    "        Categorical, and Timestamp values will happen automatically. Keep in mind the dataset can also be loaded \n",
    "        by means of methods in the base Anomaly class.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data_init: Pandas DataFrame - The dataset you would like to use for the purposes of the Report\n",
    "        autoInit: Boolean  - A True/False value deciding whether split into Categorical/Numeric/Datetime columns will be automated (default = True: automated)\n",
    "        \"\"\"\n",
    "        self.data = dataInit\n",
    "        if(autoInit == True):\n",
    "            self._split_columns()\n",
    "    \n",
    "    \n",
    "    # Default splitter for columns if user does not want to initialize\n",
    "    def _split_columns(self):\n",
    "        \"\"\"A default automatic column splitter that splits into Datetime, Numerical, and Categorical Columns \"\"\"\n",
    "        # Temporarily replacing NaN's that could screw up proccess \n",
    "        temp = self.data.replace({np.nan: 0, \"NaN\": 0})\n",
    "        numCols = []\n",
    "        catCols = []\n",
    "        dateTime = []\n",
    "        \n",
    "        # All Numerical types assigned to numCols\n",
    "        numCols = temp.select_dtypes(include = [np.number])\n",
    "        dateTime = temp.select_dtypes(include = [np.datetime64, np.timedelta64])\n",
    "        catCols = temp.select_dtypes(exclude = [np.number, np.datetime64, np.timedelta64])\n",
    "        \n",
    "        # Turning kept columns into a dictionary\n",
    "        self.colDist = {\"Numeric\": numCols.columns.to_list(),\n",
    "                        \"Categorical\": catCols.columns.to_list(),\n",
    "                        \"Timestamp\": dateTime.columns.to_list()} \n",
    "    \n",
    "    \n",
    "    # Manual user column splitter\n",
    "    def split_columns(self, numericCols:[] = [], categoricalCols:[] = [], dateTimeCols:[] = []) -> {}:\n",
    "        \"\"\"\n",
    "        A user initializer for the Numerical, Categorical, and Timestamp columns: in case user doesn't want them initialized by default.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        numericCols: list - A list of the numeric columns in the dataset. Categorical columns with numeric representations can be excluded.\n",
    "        categoricalCols: list - A list of the Categorical Columns in the Dataset\n",
    "        dateTimeCols: list - A list of timestamp/datetime columns in the dataset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        colDist: Dictionary\n",
    "            A dictionary representing the distribution of columns into numeric, categorical, and datetime\n",
    "        \"\"\"\n",
    "        self.colDist = {\"Numeric\": numericCols,\n",
    "                        \"Categorical\": categoricalCols,\n",
    "                        \"Timestamp\": dateTimeCols}\n",
    "        return self.colDist\n",
    "     \n",
    "        \n",
    "    def metadata(self) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Returns a set of tables describing Numeric and Categorical metadata (basic metrics)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numDescribe, categoricalDescribe: tuple(Pandas DataFrame, Pandas DataFrame)\n",
    "            A tuple consisting of Numeric and Categorical metadata tables.\n",
    "        \"\"\"\n",
    "        # Describing Numerical values\n",
    "        numDescribe = self.data[self.colDist[\"Numeric\"]].describe()\n",
    "        \n",
    "        # Describing Categorical values\n",
    "        categoricalDescribe = self.data[self.colDist[\"Categorical\"]].describe()\n",
    "        return numDescribe, categoricalDescribe\n",
    "    \n",
    "    \n",
    "    def run_intial_tests(self, index: str) -> (pd.DataFrame, pd.DataFrame, [pd.DataFrame]):\n",
    "        \"\"\"\n",
    "        Runs Categorical Distribution test on Categorical Column, Kernel-PCA on numeric values, and intervals on Datetime values\n",
    "        \n",
    "        Common Issues: DataFrame should preferably have a set index, which should be passed to this method.\n",
    "        This will avoid value loss at runtime.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        index: String - A String to use as an index for the Numerical Anomaly table\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        inital_report: tuple(Pandas DataFrame, pandas DataFrame, [Pandas DataFrame])\n",
    "            A DataFrame for the Categorical anomaly approximation, another one for Kernel-PCA anomaly approximation (for Numerical values)\n",
    "            and a list of DataFrames for every datetime interval anomaly approximation (tends to consist of just one element)\n",
    "        \"\"\"\n",
    "        interval_list = []\n",
    "        # Obtaining all Numeric values\n",
    "        numColFrame = self.data[self.colDist[\"Numeric\"]]\n",
    "        \n",
    "        # Obtaining all Categorical values\n",
    "        categoricalFrame = self.data[self.colDist[\"Categorical\"]]\n",
    "        \n",
    "        # Instanciating Categorical and Kernel-PCA objects\n",
    "        self.categorical = Categorical(categoricalFrame)\n",
    "        self.knalPca = KernelPCA(numColFrame)\n",
    "        \n",
    "        # Runnning individual surprise approximation on Numerical and Categorical Columns\n",
    "        categoricalFrame = self.categorical.surprise_on_table(self.categorical.data, True)\n",
    "        numColFrame = self.knalPca.run_on_current_data(index)\n",
    "        \n",
    "        # Running interval Approximations on Surprise Columns, appending them to list to be returned separately\n",
    "        for i in self.colDist[\"Timestamp\"]:\n",
    "            interval_list.append(self.knalPca.date_time_interval_anomaly(self.data[i]))\n",
    "            \n",
    "        self.report = categoricalFrame, numColFrame, interval_list\n",
    "        return self.report\n",
    "\n",
    "    \n",
    "    def basic_visuals(self, columnType: str = \"Numeric\", plotType: str  = \"hist\"):\n",
    "        \"\"\"\n",
    "        Plots columns of columnType as a plot of plotType\n",
    "        \n",
    "        Common Issues: Most Pandas plot do not handle Categorical Data very well. plotTypes are also limited to the following \n",
    "        list: \n",
    "        { line’ : line plot (default), ‘bar’ : vertical bar plot, ‘barh’ : horizontal bar plot, ‘hist’ : histogram,\n",
    "        ‘box’ : boxplot,‘kde’ : Kernel Density Estimation plot, ‘density’ : same as ‘kde’, ‘area’ : area plot, pie’ : pie plot}\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        columnType: String - The column type to plot. Options = (\"Numeric\", \"Categorical\", \"Timestamp\") (default = \"Numeric\")\n",
    "        plotType: String - The type of plot to use. Options = (\"line\", \"hist\", \"barh\", \"bar\", \"pie\"...(see more under common Issues)) (default = \"hist\")\n",
    "        \"\"\"\n",
    "        for col in self.colDist[columnType]:\n",
    "            print(\"Column: \" + str(col))\n",
    "            try:\n",
    "                self.data[col].plot(kind = plotType, legend = True)\n",
    "                \n",
    "            # If user attempts to numerically plot categorical data\n",
    "            except TypeError:\n",
    "                print(\"Error: Certain plots require numeric data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: add Label learning for anomaly: cluster: have the user label a couple instances, and then proceed to calculate anomaly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
