{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for project purposes\n",
    "# Full Project imports\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import dateutil\n",
    "from datetime import datetime, timedelta\n",
    "import requests as rd\n",
    "import numpy as np\n",
    "from sklearn import neighbors, decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import smtplib\n",
    "import scipy.stats as st\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anomaly:\n",
    "    '''Base Class for an anomaly detection method'''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    \n",
    "    # Constructor to set values for data\n",
    "    def __init__(self, input_data = None):\n",
    "        '''\n",
    "        Constructor for setting dataset reference to a specific dataset\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data: Pandas DataFrame reference - Your dataset in the form of a Pandas DataFrame\n",
    "        '''\n",
    "        # Ensuring data is properly formatted\n",
    "        assert input_data is None or type(input_data) is pd.core.frame.DataFrame, \"inputted data is not a pandas DataFrame\"\n",
    "        self.data  = input_data\n",
    "        \n",
    "        \n",
    "    # Loading data into project\n",
    "    def load_html(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an HTML table and sets it as the dataset for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_html(link)\n",
    "        return self.data    \n",
    "    \n",
    "    \n",
    "    # Loading data into project\n",
    "    def load_csv(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an CSV table and sets it as the dataset for the model. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_csv(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_excel(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads an Exel table and sets it as the dataset for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_excel(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    #Loading data into project\n",
    "    def load_sql_table(self, link: str) -> pd.DataFrame():\n",
    "        '''\n",
    "        Loads a SQL table and sets it as the dataset for the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        link: String - The link to the dataset that is being loaded\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the entire DataFrame that has just been loaded as the dataset for the bootstrap model\n",
    "        '''\n",
    "        self.data = pd.read_sql_table(link)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Setter for the training set\n",
    "    def set_train(self, trainingSet: pd.DataFrame):\n",
    "        '''\n",
    "        A setter for the training set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        trainingSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        '''\n",
    "        self.trainDf = trainingSet\n",
    "    \n",
    "    \n",
    "    # Setter for the test set\n",
    "    def set_test(self, testSet: pd.DataFrame):\n",
    "        '''\n",
    "        A setter for the test set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        testSet: pandas DataFrame - A DataFrame object that will serve as your training set\n",
    "        '''\n",
    "        self.testDf = testSet\n",
    "        \n",
    "    \n",
    "    # Randomly split train and test set\n",
    "    def assign_train_test(self,random_state = 42, training_set_ratio = 0.8, shuffling = True):\n",
    "        '''\n",
    "        A default random splitter into train and test set\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int - determines random state fed to model for reproducability of random results, default is 42\n",
    "        training_set_ratio: float between 0.0 and 1.0 - what % of your data you would like to encompass the training set (test set will be made in complimentary way) default is 0.8\n",
    "        shuffling: boolean - whether or not you would like your data randomly shuffled out of chronology prior to split (True/False). Default is True.\n",
    "        '''\n",
    "        # Ensuring that data actually exists before splitting\n",
    "        assert not(self.data is None), \"You cannot assign a train and test set out of a dataset that has not been initialized\"\n",
    "        \n",
    "        # Splitting into train and test\n",
    "        self.trainDf, self.testDf = train_test_split(self.data, train_size = training_set_ratio, shuffle = shuffling)\n",
    "        return self.trainDf, self.testDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrap(Anomaly):\n",
    "    '''A class for returning anomaly of categorical column counts, utilizing the metric of surprise (entropy)'''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    timestamp = None\n",
    "    params = None\n",
    "    \n",
    "    # Overloaded constructor in case user doesn't want to fit data right away\n",
    "    def __init__(self, timeCol = 'date_time', resamples = 1000, maxTrainingSizeMult = 10, maxCategory = 100, minCategories = 10):\n",
    "        '''\n",
    "        Constructor which does not require immediate fit to model, merely initializes timestamp if given\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timeCol: String - The name of the primary TimeStamp column\n",
    "        resamples: int - the number of times the bootstrap resamples. Making this very large will improve accuracy but significantly lower speed. Default = 1000\n",
    "        maxTrainingSizeMult: int - If there is more than x  = maxTrainingSizeMult ratio of training to test data, trim training data to most recent. Default = 10\n",
    "        maxCategory: int - Maximum number of categories in a column (to ensure that counts are not tiny and are meaninful), column skipped if value count higher than this. Default = 100\n",
    "        minCategory: int - if column has a category count that is lower than this value, don't report it in bootstrap surprise. Default = 10.\n",
    "        '''\n",
    "        # Initializing time\n",
    "        timestamp = timeCol\n",
    "        \n",
    "        # Meta-parameter initialization\n",
    "        params = {\n",
    "          \"bootstrapResamples\": resamples,\n",
    "          \"maxTrainingSizeMultiple\":maxTrainingSizeMult, # if there is more than X times more training data, trim to most recent\n",
    "          \"maxCategories\":maxCategory,\n",
    "          \"minCategoryCount\": minCategories,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # Fot fitting data right away\n",
    "    def __init__(self, dataset, timeCol = \"date_time\",  resamples = 1000, maxTrainingSizeMult = 10, maxCategory = 100, minCategories = 10):\n",
    "        '''\n",
    "        Overloaded constructor for attaching dataset immediately, can be done independently within any of the load functions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: String - A pandas data frame reference\n",
    "        timeCol: String - The name of the primary TimeStamp column. Default = \"date_time\".\n",
    "        resamples: int - the number of times the bootstrap resamples. Making this very large will improve accuracy but significantly lower speed. Default = 1000\n",
    "        maxTrainingSizeMult: int - If there is more than x  = maxTrainingSizeMult ratio of training to test data, trim training data to most recent. Default = 10\n",
    "        maxCategory: int - Maximum number of categories in a column (to ensure that counts are not tiny and are meaninful), column skipped if value count higher than this. Default = 100\n",
    "        minCategory: int - if column has a category count that is lower than this value, don't report it in bootstrap surprise. Default = 10.\n",
    "        '''\n",
    "        timestamp = timeCol\n",
    "        data = dataset\n",
    "        \n",
    "        # Meta-parameter initialization\n",
    "        params = {\n",
    "          \"bootstrapResamples\": resamples,\n",
    "          \"maxTrainingSizeMultiple\":maxTrainingSizeMult, # if there is more than X times more training data, trim to most recent\n",
    "          \"maxCategories\":maxCategory,\n",
    "          \"minCategoryCount\": minCategories,\n",
    "        }\n",
    "        \n",
    "    \n",
    "    # Converts Timetamp column of DataFrame to a legitimate timestamp\n",
    "    def convert_time_stamp_to_datetime(self: str, formatting = '%Y%m%d %H:%M:%S') -> pd.DataFrame:\n",
    "        '''\n",
    "        Converts a chosen timestamp column from string to date/time, making the modifications both to the fitted\n",
    "        Data Frame and returning the new Data Frame\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp: String - The name of the Timestamp column that needs conversion\n",
    "        formatting: String - If formatting different from default = %Y%m%d %H:%M:%S, enter the format of your TimeSeries column\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        data : pandas.DataFrame\n",
    "            Returns the locally the entire DataFrame with the modified Timestamp column\n",
    "        '''\n",
    "        self.data[timestamp] =  pd.to_datetime(self.data[timestamp], format = formatting)\n",
    "        return self.data\n",
    "    \n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(batchHours = 24*7):\n",
    "        '''\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        '''\n",
    "        maxTs = max(self.data[timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[timestamp] < batchTs]\n",
    "        \n",
    " \n",
    "    # Helpers and Math\n",
    "    def pValue(self,data, threshold: np.number, result: pd.DataFrame) -> np.array:\n",
    "        '''\n",
    "        Returns the p-value of a computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Pandas DataFrame - The Data we are computing the P-value on\n",
    "        threshold: np.number - The threshold to check if data is anomalous\n",
    "        result: pd.DataFrame - A DataFrame containing the column \"Bootstrap counts\" to be normalized and tested for anomaly\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pGauss : np.array\n",
    "            Returns the array of normalized p-values for each bootstrap count\n",
    "        '''\n",
    "        # Taking the smaller of the 2 p-values(either could present large anomaly)\n",
    "        pLarger = sum(np.array(data) >= threshold) / len(data)\n",
    "        pSmaller = sum(np.array(data) <= threshold) / len(data)\n",
    "        p = min(pLarger, pSmaller)\n",
    "\n",
    "        # only use gaussian p-value when there is variation, but bootsrap p = 0\n",
    "        stdev = np.std(data)\n",
    "        if stdev == 0 or p != 0:\n",
    "            pGauss = p\n",
    "        else:\n",
    "            # Normalizing\n",
    "            pGauss = st.norm(np.mean(result['bootstrap_counts']), stdev).cdf(result['count'])\n",
    "            pGauss = min(pGauss,1-pGauss)\n",
    "        return pGauss\n",
    "\n",
    "    \n",
    "    def trimFrame(self,df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Trims a DataFrame, ensuring that it does not exceed the training set max size hyper parameter\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame that is being trimmed to fit to the training set hyperparameter\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dfTrimmed : pandas DataFrame\n",
    "            Returns a DataFrame fit to the training set specifications\n",
    "        '''\n",
    "        # trim to most recent\n",
    "        df = df.sort_values(self.timestamp, ascending =False)\n",
    "        dfTrimmed = df[:self.params['maxTrainingSizeMultiple']*len(testDf)]\n",
    "\n",
    "        return dfTrimmed\n",
    "    \n",
    "    \n",
    "    # Returns names of categorical columns\n",
    "    def getCategoricalColumnNames(df: pd.DataFrame) -> []:\n",
    "        '''\n",
    "        Returns the names of categorical columns in a Pandas DataFrame (if the type is a string)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pandas DataFrame - The DataFrame whose columns are checked for being categorical data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        columnNames : list\n",
    "            The list of all categorical column names \n",
    "        '''\n",
    "        columnNames = []\n",
    "        for columnName in df.keys():\n",
    "            if (type (df[columnName].iloc[0])) == str:\n",
    "                columnNames.append(columnName)\n",
    "        return columnNames\n",
    "    \n",
    "    \n",
    "    def train_test_anomaly(self) -> pd.DataFrame:\n",
    "        '''\n",
    "        Tests for difference between training and test set counts, returning a report that quantifies difference between\n",
    "        training and test set as surprise.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        resultsDf : pandas DataFrame\n",
    "            A DataFrame containing a report for the difference between expected and detected counts within the test set \n",
    "            With the inclusion of a column quantifying irregularity as surprise (entropy)\n",
    "        \n",
    "        '''\n",
    "        # get all of the string columns\n",
    "        columnNames = self.getCategoricalColumnNames(self.testDf)\n",
    "\n",
    "        bootstrapDf = self.trimFrame(self.trainDf)\n",
    "\n",
    "        # set up dict, add counts\n",
    "        results = {}\n",
    "\n",
    "\n",
    "        for columnName in columnNames:\n",
    "\n",
    "            # if it isn't a string column, reject it\n",
    "            if type(testDf[columnName].iloc[0]) != str:\n",
    "                continue\n",
    "            categories = (bootstrapDf[columnName].append(self.testDf[columnName])).unique()\n",
    "            if len(categories) > self.params['maxCategories']:\n",
    "                continue\n",
    "\n",
    "            results[columnName] = {}\n",
    "            testCounts = self.testDf[columnName].value_counts(dropna = False)\n",
    "            \n",
    "            \n",
    "            for i in np.arange(1,len(categories) -1):\n",
    "                if(pd.isna(categories[i])):\n",
    "                    categories = np.delete(categories, i)  \n",
    "            for category in categories:\n",
    "                results[columnName][category] = {'bootstrap_counts':[],\n",
    "\n",
    "                                                 'count':testCounts.get(category,0)}\n",
    "        # resample, add boostrap counts\n",
    "        for ii in range(params['bootstrapResamples']):\n",
    "\n",
    "            # Draw random sample from training\n",
    "            sampleDf = bootstrapDf.sample(len(testDf), replace=True)\n",
    "            for columnName in results.keys():\n",
    "\n",
    "                # count by category\n",
    "                trainCounts = sampleDf[columnName].value_counts(dropna = False)\n",
    "\n",
    "                # put results in dict\n",
    "                for category in results[columnName].keys():\n",
    "                    boostrapCount = trainCounts.get(category,0)\n",
    "                    results[columnName][category]['bootstrap_counts'].append(boostrapCount)\n",
    "\n",
    "        # convert to records, add p-values\n",
    "        bootstrap_results = []\n",
    "        for columnName in results.keys():\n",
    "            for category in results[columnName].keys():\n",
    "                result = results[columnName][category]\n",
    "\n",
    "                estimatedCount = int(np.round(np.mean(result['bootstrap_counts'])))\n",
    "\n",
    "                # don't report entries with very low predicted and actual counts\n",
    "                if estimatedCount < params['minCategoryCount'] and result['count'] < params['minCategoryCount']:\n",
    "                    continue\n",
    "\n",
    "                p = pValue(result['bootstrap_counts'],result['count'], result)\n",
    "                categoryName = category\n",
    "\n",
    "                # Backup\n",
    "                if not category:\n",
    "                    categoryName = \"NULL\"\n",
    "\n",
    "                bootstrap_results.append({\"column\":columnName,\n",
    "                                   \"category\":categoryName,\n",
    "                                   \"count\":result['count'],\n",
    "                                   \"p\": p,\n",
    "                                   \"estimated_count\":estimatedCount,\n",
    "                                   })\n",
    "\n",
    "        # Sorting by P-values and obtaining Surprise of each\n",
    "        if(np.count_nonzero(p)>0):\n",
    "            resultsDf = pd.DataFrame.from_records(bootstrap_results).sort_values('p')\n",
    "            resultsDf['surprise'] = -np.log2(resultsDf['p'])\n",
    "\n",
    "            return resultsDf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeries(Anomaly):\n",
    "    '''\n",
    "    Utilizes facebook prophet and its ability to predict the future based off specific time context (day, hour, holiday)\n",
    "    to make predictions and test those against the dataset, thus finding anomaly with the context of time\n",
    "    '''\n",
    "    data = None\n",
    "    trainDf = None\n",
    "    testDf = None\n",
    "    \n",
    "    def __init__(self,inp_data = None):\n",
    "        from fbprophet import Prophet\n",
    "        data  = inp_data\n",
    "            \n",
    "    def truncateTs(ts):\n",
    "        return ts.replace(minute=0, second=0,  microsecond=0)\n",
    "    \n",
    "    def group_and_build_time_table(truncatedData):\n",
    "        groupedCounts = truncatedData.value_counts()\n",
    "        prophetDf = pd.DataFrame({'ds':groupedCounts.index,'y':np.log10(groupedCounts.values)})\n",
    "        return prophetDf\n",
    "    \n",
    "    # Takes in the the dataset and the prophet dataset returned by the ast option\n",
    "    def train_model_on_country(testDf, prophetDf, country = \"US\"):\n",
    "        # Train model\n",
    "        m = Prophet(#daily_seasonality = True, \n",
    "                    #yearly_seasonality = False, \n",
    "                    #weekly_seasonality = True, \n",
    "                    #growth='linear',\n",
    "                    interval_width=0.68 # one sigma\n",
    "                   )\n",
    "        m.add_country_holidays(country_name=country)\n",
    "\n",
    "        m.fit(prophetDf)\n",
    "        return m\n",
    "    \n",
    "    # Splits data into train and test set based on date/time\n",
    "    def split_train_test_by_time(batchHours = 24*7):\n",
    "        '''\n",
    "        Splits Data into a train and test set, held within the object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batchHours: int - Size of the test set in terms of hours. Default is one week (24 * 7).\n",
    "        '''\n",
    "        maxTs = max(self.data[timestamp])\n",
    "        batchTs = maxTs - timedelta(hours = batchHours)\n",
    "        self.testDf = self.data[self.data[timestamp] > batchTs]\n",
    "        self.trainDf = self.data[self.data[timestamp] < batchTs]\n",
    "        \n",
    "\n",
    "    # Applies Prophet analytics to create a forecast based on hours\n",
    "    def predict_future(testDf,m, timestamp = \"date_time\"):\n",
    "\n",
    "        # Takes in trained model and predicts the future\n",
    "        # find number of hours to preduct: ceil of hours in testDf\n",
    "        testDf = testDf.assign(ts = testDf.get(timestamp))\n",
    "\n",
    "        #If a column is string, convert to date/time\n",
    "        if(testDf.applymap(type).eq(str).any()['ts']):\n",
    "            testDf['ts'] = pd.to_datetime(testDf['ts'])\n",
    "\n",
    "        timeDelta = max(testDf['ts']) -min(testDf['ts'])\n",
    "        hours = int(timeDelta.days*24 + timeDelta.seconds/(60*60))+1\n",
    "        future = m.make_future_dataframe(periods = hours, freq = 'H')\n",
    "        forecast = m.predict(future)\n",
    "        return forecast, testDf\n",
    "\n",
    "    def find_surprise(truncatedData, forecast):\n",
    "    groupedCounts = truncatedData.value_counts()\n",
    "\n",
    "    prophetTestDf = pd.DataFrame({'ds':groupedCounts.index,\n",
    "                                  'y':np.log10(groupedCounts.values),\n",
    "                                  'y_linear':groupedCounts.values})\n",
    "\n",
    "    # find p-value\n",
    "    prophet_results = []\n",
    "\n",
    "    # Comparing test and training set data for identical intervals\n",
    "    for ii in range(len(prophetTestDf)):\n",
    "        ts = prophetTestDf['ds'][ii]\n",
    "        fcstExample = forecast[forecast['ds'] == ts]\n",
    "        mean = fcstExample['yhat'].iloc[0]\n",
    "        stdev = (fcstExample['yhat_upper'].iloc[0] - fcstExample['yhat_lower'].iloc[0])/2\n",
    "        \n",
    "        # Calculating the P-value\n",
    "        p = st.norm(mean, stdev).cdf(prophetTestDf['y'][ii])\n",
    "        p = min(p,1-p)\n",
    "\n",
    "        prophet_results.append({\"column\":\"Forecast\",\n",
    "                           \"category\":str(ts),\n",
    "                           \"count\":prophetTestDf['y_linear'][ii],\n",
    "                           \"p\": p,\n",
    "                           \"estimated_count\":int(np.round(np.power(10,mean))),\n",
    "                           })\n",
    "    \n",
    "    # Obtaining Entropy of Time-Series values\n",
    "    prophetResultsDf = pd.DataFrame.from_records(prophet_results).sort_values('p')\n",
    "    prophetResultsDf['surprise'] = -np.log2(prophetResultsDf['p'])\n",
    "    return prophetResultsDf\n",
    "\n",
    "    # Takes in a model that has been trained on country, plots graphs for visualization\n",
    "    def visualize(m, forecast):\n",
    "        # Model visualization\n",
    "        fig = m.plot(forecast)\n",
    "        fig = m.plot_components(forecast)\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPCA(Anomaly):\n",
    "    '''Combines Kernel Density and PCA into a join proccess that runs on all numerical columns to triangulate outliers'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(Anomaly):\n",
    "    '''Column-based numerical outlier tester that utilizes fitting a Kernel and obtaining a density estimation'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(Anomaly):\n",
    "    '''Row-based outlier techniques that utilizes dimensionality reduction to understand systematic bias by row'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical(Anomaly):\n",
    "    '''Uses dynamically built data \"grammar conventions\" to find outliers based on defiance of strict structures'''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDimCategorical(Anomaly):\n",
    "    '''\n",
    "    Utilizes the idea of mutual entropy to build first order and 2nd order approximations for a \n",
    "    given column based on randomly chosen/handpicked context\n",
    "    '''\n",
    "    def __init__(self,x):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunInitial:\n",
    "    '''A simple base class for an initial non-verbose outlier scan'''\n",
    "    data = None\n",
    "    def __init__(self,x):\n",
    "        print(x)\n",
    "        \n",
    "    def categorical_column_anomaly():\n",
    "        pass\n",
    "    \n",
    "    def numerical_column_anomaly():\n",
    "        pass\n",
    "        \n",
    "    def date_time_anomaly():\n",
    "        pass\n",
    "    \n",
    "    def obtainAnomaly():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report(RunInitial):\n",
    "    '''An extention of the RunInitial class that offers a more verbose and visual report for an initial anomaly scan'''\n",
    "    data = None\n",
    "    \n",
    "    def __init__(self,x):\n",
    "        \n",
    "    def metadata(self):\n",
    "        pass\n",
    "    \n",
    "    def design_report(self):\n",
    "        pass\n",
    "        \n",
    "    def report_to_excel(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_pca(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_kernel_density(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_decision_tree(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
